<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="will"><meta name="keywords" content="will"><meta name="description" content="本章以开源&#x2F;闭源模型为划分，介绍一下日常使用及评估的经验。可能较为主观，当然一些介绍就直接照搬了，这里不做过多的赘述，请各位看官也要多多结合自身体感及实际业务体验来评判。  闭源模型闭源模型：一种循环 目前实现了SOTA（State of the Art，特定领域或任务中，当前的最新进展和最高水准，基本上是各家自称）的闭源模型厂主要有如下几家（豆包除外，稍后单讲）：    公司&amp;#x2"><meta property="og:type" content="article"><meta property="og:title" content="各大常用模型特点"><meta property="og:url" content="https://github.com/yangxiangnanwill/yangxiangnanwill.github.io/2026/01/05/%E5%A5%BD%E5%A5%BD%E7%A0%81%E4%BB%A3%E7%A0%81%E5%90%96/LLM/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B/%E5%90%84%E5%A4%A7%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9/index.html"><meta property="og:site_name" content="Will"><meta property="og:description" content="本章以开源&#x2F;闭源模型为划分，介绍一下日常使用及评估的经验。可能较为主观，当然一些介绍就直接照搬了，这里不做过多的赘述，请各位看官也要多多结合自身体感及实际业务体验来评判。  闭源模型闭源模型：一种循环 目前实现了SOTA（State of the Art，特定领域或任务中，当前的最新进展和最高水准，基本上是各家自称）的闭源模型厂主要有如下几家（豆包除外，稍后单讲）：    公司&amp;#x2"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2026-01-05T02:30:52.535Z"><meta property="article:modified_time" content="2026-01-05T04:07:33.939Z"><meta property="article:author" content="will"><meta property="article:tag" content="LLM"><meta property="article:tag" content="VLM"><meta name="twitter:card" content="summary_large_image"><title>各大常用模型特点 - Will</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var dntVal,Fluid=window.Fluid||{},CONFIG=(Fluid.ctx=Object.assign({},Fluid.ctx),{hostname:"github.com",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"});CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.4.2"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>WILL</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="各大常用模型特点"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2026-01-05 10:30" pubdate>2026年1月5日 上午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 12k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 103 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">各大常用模型特点</h1><div class="markdown-body"><blockquote><p>本章以开源&#x2F;闭源模型为划分，介绍一下日常使用及评估的经验。可能较为主观，当然一些介绍就直接照搬了，这里不做过多的赘述，请各位看官也要多多结合自身体感及实际业务体验来评判。</p></blockquote><h1 id="闭源模型"><a href="#闭源模型" class="headerlink" title="闭源模型"></a>闭源模型</h1><p><strong>闭源模型：一种循环</strong></p><p>目前实现了SOTA（<strong>State</strong> <strong>of</strong> <strong>the</strong> <strong>Art</strong>，特定领域或任务中，当前的最新进展和最高水准，基本上是各家自称）的闭源模型厂主要有如下几家（豆包除外，稍后单讲）：</p><table><thead><tr><th>公司&#x2F;机构</th><th>AI 模型系列</th></tr></thead><tbody><tr><td>OpenAI</td><td>GPT系列</td></tr><tr><td>Google</td><td>Google Gemini系列</td></tr><tr><td>Anthropic</td><td>Claude系列</td></tr><tr><td>xAI</td><td>Grok系列</td></tr><tr><td>阿里巴巴</td><td>通义千问系列</td></tr><tr><td>字节跳动</td><td>豆包系列</td></tr></tbody></table><p>这几家基本上每隔一段时间就宣称自己发布了最强大的xx模型，以至于形成了一种循环。当然SOTA这个词很微妙，最新最大杯的模型未必就最适合你。下面按照模型家族介绍一下本代的各种主力型号的特点：</p><h2 id="OpenAI-GPT：冷静的理性思考"><a href="#OpenAI-GPT：冷静的理性思考" class="headerlink" title="OpenAI GPT：冷静的理性思考"></a>OpenAI GPT：冷静的理性思考</h2><p>自从迈入GPT-5时代以来，GPT系列模型就以回复简短闻名。从好的方面看，OpenAI做到了省output token（输出token数），这使得任务总体所需时间进一步得到压缩。然而代价是冷漠到近乎不近人情的回复使得创意写作用户不得不忍痛抛弃它。后续推出的编码特化模型<code>gpt5-codex</code>模型进一步强化了这个特征，有时候描述性文字几乎已经不能称之为人话了。好在GPT-5.2系列在一定程度上解决了这个问题，虽然比起GPT-4.5甚至GPT-4o系列模型给人在Chat上的主观感受仍有差距，但已经较为可用。</p><p>OpenAI作为LLM的领头羊，服务器的压力自然是很大的，无论是网页还是API都可能会有服务异常的情况。为了解决这个问题，GPT-5系列在网页端给出的解决方案是自动路由（其实就是超级降智）。然而，对于指定了特定型号的API用户来说，GPT-5系列模型的推理速度仍然显得相对较慢。</p><p>说完了缺点，那么剩下的基本上全是优点。回复简短意味着完成同等任务下所需tokens更少，冷静的理性思考带给人一种指哪打哪的感觉——不废话，just do it。比起GPT-4时代的人味儿来说，GPT-5更像一名理工男。当然，它是一名后端理工男，在审美上未必有多好的品味。</p><table><thead><tr><th>模型名称</th><th>模型 ID</th><th>上下文长度</th><th>最大输出长度</th><th>备注</th></tr></thead><tbody><tr><td><strong>GPT-5.2 Thinking</strong></td><td><code>gpt-5.2</code>（<code>gpt-5.2-2025-12-11</code>）</td><td>400K</td><td>128K</td><td>最高推理强度，支持 reasoning 参数（大杯）</td></tr><tr><td><strong>GPT-5.2 Pro</strong></td><td><code>gpt-5.2-pro</code></td><td>400K</td><td>128K</td><td>企业级最高准确度，支持 xhigh reasoning（超大杯）</td></tr><tr><td><strong>GPT-5.2 Chat (Instant)</strong></td><td><code>gpt-5.2-chat-latest</code></td><td>128K</td><td>16K</td><td>ChatGPT“GPT-5.2 即时”模式，延迟最低（其实就是小杯，很蠢）</td></tr><tr><td><strong>GPT-5.2 (base)</strong></td><td><code>gpt-5.2</code></td><td>400K</td><td>128K</td><td>通用旗舰版，默认 reasoning&#x3D;medium（中杯）</td></tr><tr><td><strong>GPT-5.2-Codex</strong></td><td><code>gpt-5.2-codex</code></td><td>400K</td><td>128K</td><td>代理式编码专用，支持上下文压缩与视觉输入</td></tr><tr><td><strong>GPT-5.1-Codex-Max</strong></td><td><code>gpt-5.1-codex-max</code></td><td>400K</td><td>128K</td><td>支持“压缩”技术，可跨多窗口连贯处理数百万 tokens，专为长时间、项目级编码任务设计</td></tr></tbody></table><p>这里需要特别注意的是，<code>gpt-5.2-codex</code>并非代码万灵药。如果你不太会写prompt或者这个工程需要范围更广的探索思考，那么<code>gpt-5.2</code>可能会比codex变体好用些。codex更突出指哪打哪的能力，而<code>gpt-5.2</code>会主动帮你多想些。换句话说，改bug用<code>gpt-5.2-codex</code>，新开工程&#x2F;模块用<code>gpt-5.2</code>。推荐写后端或复杂的前端逻辑时使用GPT系列模型。</p><h2 id="Gemini：多模态和世界知识之王"><a href="#Gemini：多模态和世界知识之王" class="headerlink" title="Gemini：多模态和世界知识之王"></a>Gemini：多模态和世界知识之王</h2><p>老谷坐拥无尽的网络资源宝库以及Deepmind+TPU的神秘力量加持，尽管在LLM时代赶了个晚集，但从Gemini 2.0开始一路猛追，到了2.5时代已经是妥妥的御三家之一。Gemini的多模态能力令人惊叹，Pro系列的世界知识更是让人折服。比起GPT来说，Gemini更像一名文科生：大参数带来的丰富世界知识给了它更强的文学理解能力，思考之细腻和情感共鸣能力使得它成为创意写作的最优选。当接入Chatbot的时候，你甚至可能没法分清它到底是AI还是人——太能接梗了。</p><p>大家都不知道Gemini Pro系列的参数到底有多大，目前普遍认为1T以上。然而推理速度比起其他各家大参数模型来说又快的离谱，疑似Jeff Dean在机房里手敲（其实应该是TPU的特点所致）。总之，如果你想选择一款有超强的世界知识并且对推理速度有一定要求的模型，那么Gemini系列是毋庸置疑的选择。</p><p>Gemini 3.0 Pro从内部测试阶段就不断炸场，多模态+大参数写出的前端效果惊艳了所有关注AI前沿动向的人。尽管Gemini 3.0 Pro存在较为严重的长上下文幻觉问题，但瑕不掩瑜，它依然是现在最适合前端的模型。</p><p>Gemini 3.0 Flash推出后，甚至神秘地实现了某种程度上对Pro的反杀，几乎和Pro一样丰富的世界知识和更好的编码能力。下克上？搞不懂老谷。</p><table><thead><tr><th><strong>模型名称</strong></th><th><strong>模型 ID</strong></th><th><strong>上下文长度</strong></th><th><strong>最大输出长度</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>Gemini 3 Pro</strong></td><td><code>gemini-3-pro</code></td><td>1000K (1M)</td><td>64K</td><td><strong>旗舰模型</strong>。最强多模态推理与编码能力，支持 <code>high</code> 深度思维模式。前端很强非常强！但受限于长上下文幻觉，后端稀烂（相比其他两家）</td></tr><tr><td><strong>Gemini 3 Flash</strong></td><td><code>gemini-3-flash</code></td><td>1000K (1M)</td><td>64K</td><td><strong>速度旗舰</strong>。专为 Agent 设计，支持 <code>minimal/medium</code> 等多级思维调节。Flash反杀Pro！大部分搬砖的活计用Flash就够了，速度飞快。</td></tr><tr><td><strong>Gemini 2.5 Pro</strong></td><td><code>gemini-2.5-pro</code></td><td>1000K (1M)</td><td>64K</td><td>2.5 世代旗舰。具备极强的长文本召回能力。（前面是官方说法，实际上各家长文本都一坨）</td></tr><tr><td><strong>Gemini 2.5 Flash</strong></td><td><code>gemini-2.5-flash</code></td><td>1000K (1M)</td><td>64K</td><td>2.5 世代均衡版。高吞吐量，默认支持长上下文处理。</td></tr><tr><td><strong>Gemini 2.5 Flash-Lite</strong></td><td><code>gemini-2.5-flash-lite</code></td><td>1000K (1M)</td><td>64K</td><td><strong>极致性价比</strong>。针对极低延迟任务优化，是目前最廉价的百万上下文模型。</td></tr></tbody></table><h2 id="Anthropic-Claude：最均衡的编码代理模型"><a href="#Anthropic-Claude：最均衡的编码代理模型" class="headerlink" title="Anthropic Claude：最均衡的编码代理模型"></a>Anthropic Claude：最均衡的编码代理模型</h2><p>Anthropic，又称A÷ &#x2F; A畜，大家很熟悉了，神一样的Coding Agent，翔一样的口碑和服务可用性。抛开立场不谈，最早的Claude模型以创意写作闻名，比起同期的GPT-3.5来说回答更有人味。后来Claude率先扩展了长上下文窗口以及STEM能力，走向了编码特化的不归路。到了Claude 3时代开始就是彻头彻尾的Coding模型了，直到现在的Claude 4.5成为了最均衡的编码代理模型——如果你想前后端一把抓，选它准没错。强大的规划能力能够给出更适合工程上的方案，在各种场景下都能很好的完成目标。跑分没赢过，体验没输过。尽管日常处于即将被超越的状态，但还没被超越不是吗？（对标苹果，友商是xx！）</p><table><thead><tr><th><strong>模型名称</strong></th><th><strong>模型 ID</strong></th><th><strong>上下文长度</strong></th><th><strong>最大输出长度</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>Claude 4.5 Opus</strong></td><td><code>claude-4-5-opus-20251124</code></td><td>200K</td><td>64K</td><td>支持 <code>effort</code> 参数调节推理强度。编码与科研任务首选（超大杯）（反重力反代优选）</td></tr><tr><td><strong>Claude 4.5 Sonnet</strong></td><td><code>claude-4-5-sonnet-20250929</code></td><td>200K &#x2F; 1000K*</td><td>64K</td><td>专为复杂 Agent 与项目级代码设计，性能超越早期 Opus 4（中杯）（对于反重力用户来说，有Opus谁用Sonnet）</td></tr><tr><td><strong>Claude 4.5 Haiku</strong></td><td><code>claude-4-5-haiku-20251014</code></td><td>200K</td><td>64K</td><td>路边一条，官方说具备 Sonnet 4 级别的性能，但被Gemini Flash家族打出shi来了</td></tr></tbody></table><p>注：只有官方Max订阅才有1000K上下文，大部分渠道都是200K的上下文，比如反重力逆向或Kiro逆向。</p><h2 id="xAI-Grok：力大砖飞，以及瑟瑟"><a href="#xAI-Grok：力大砖飞，以及瑟瑟" class="headerlink" title="xAI Grok：力大砖飞，以及瑟瑟"></a>xAI Grok：力大砖飞，以及瑟瑟</h2><p>马斯克也许缺乏品味，但他足够有钱。Grok好不好用先放一边，超大规模的显卡集群是实打实存在的。这个系列一直秉持力大砖飞的原则，猛堆参数。迫于Scaling law的存在，就算是几百头猪，炼进Transformer里也能出些成果了罢。</p><p>Grok在某些领域有着和Gemini系列相似的特性：参数够大，很适合创意写作任务。Grok 4家族拥有不俗的吐槽能力，在对齐上比起<code>a helpful assistant</code>来说更像一名沙雕网友。而且Grok背靠X（aka Twitter），也有着丰富的语料及不错的搜索功能。对于老外来说，Grok简直是全自动开盒器（is that true ? )</p><p>Grok系列另一个令人津津乐道的地方就是极低的审查下限。在各家API中，Grok &#x2F; Google Vertex &#x2F; DeepSeek是审查力度相对较低的。但到了网页端上Grok也保持极低的审查下限就很离谱，当然考虑到X网页端上你依然可以畅爽NSFW…好吧，Grok适合搞瑟瑟是从娘胎里就带出来的本事。无需破甲，无需诱导，很黄很暴力。酒馆和各种文字扮演游戏的常客。</p><table><thead><tr><th><strong>模型名称</strong></th><th><strong>模型 ID</strong></th><th><strong>上下文长度</strong></th><th><strong>最大输出长度</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>Grok 4 Heavy (SuperGrok)</strong></td><td><code>grok-4-heavy</code></td><td>256K</td><td>8K - 16K</td><td><strong>多智能体协作系统</strong>，通过并行推理验证结果，推理强度最高（超大杯）</td></tr><tr><td><strong>Grok 4.1</strong></td><td><code>grok-4.1</code></td><td>256K</td><td>16K</td><td>2025年底旗舰，主打<strong>高情商 (EQ)</strong> 与低幻觉率，创意写作能力很好（大杯）</td></tr><tr><td><strong>Grok 4</strong></td><td><code>grok-4</code></td><td>256K</td><td>8K</td><td>2025年中发布的标准旗舰，原生支持多模态推理与实时 X 搜索</td></tr><tr><td><strong>Grok 4.1 Fast (Long)</strong></td><td><code>grok-4.1-fast</code></td><td>2,000K</td><td>16K</td><td><strong>超长上下文版</strong>，支持 200 万 token，类似Gemini Flash（中杯）</td></tr><tr><td><strong>Grok 4 Fast (Instant)</strong></td><td><code>grok-4-fast</code></td><td>2,000K</td><td>30K</td><td><strong>极速&#x2F;高性价比版</strong>，支持 reasoning 切换（可关闭推理以获得极低延迟，类似Gemini Flash Lite，小杯）</td></tr><tr><td><strong>Grok Code Fast 1</strong></td><td><code>grok-code-fast-1</code></td><td>256K</td><td>16K</td><td><strong>马斯克的钞能力</strong>，在一众编程模型当中显得平平无奇，但不要钱不要钱不要钱！速度很快，质量一般，体感跟Gemini 2.5 Flash差不多的性能，但在各种 Vibe Coding 客户端里都作为免费选项出现。</td></tr></tbody></table><h2 id="阿里-通义千问-amp-字节跳动-豆包"><a href="#阿里-通义千问-amp-字节跳动-豆包" class="headerlink" title="阿里 通义千问 &amp; 字节跳动 豆包"></a>阿里 通义千问 &amp; 字节跳动 豆包</h2><blockquote><p>能力先行还是产品先行？</p></blockquote><p>阿里作为目前开源界当之无愧的扛把子，从Meta手中接过了开源的大旗。r&#x2F;LocalLlama如今已是r&#x2F;LocalQwen的形状了。Qwen家族分为开源模型和闭源模型两种。除了每代的超大杯（通义千问Max）为闭源外，其他商业API均能找到对应的类似开源型号。通义千问的特点是极强的指令遵循能力和稀烂的产品。</p><p>Qwen家族的模型在输出上总感觉缺了点味道。它不像GPT那样冷静简洁，不像Gemini那样细腻有人味，但也不像DeepSeek R1 0120那样放飞自我。很怪，AI味很重，在大规模使用RL训练的Qwen3世代这个特点尤为显著。国模的通病之一在Qwen上有显著体现：思考时非常消耗Token，甚至在Instruct模型上模型倾向于输出思维链，导致最终完成复杂任务时所耗Token相对较高。</p><p>但从另一个方面上来讲，Qwen作为国内AI的T0选手，其模型非常适合国内企业落地开发使用：性价比适中、模型选择丰富、较好的服务稳定性，还有强大的指令遵循能力可以减轻不少开发难度。逻辑能力也相当不错。</p><p>阿里系除了主打的阿里云百炼平台提供的通义千问服务外，还有面向开发者的modelscope（魔搭）、心流团队的iFlow、面向C端的蚂蚁灵光系列，主打一个养蛊和乱拳打死老师傅。以下表格主要介绍闭源的通义千问3家族：</p><table><thead><tr><th><strong>模型名称</strong></th><th><strong>模型 ID</strong></th><th><strong>上下文长度</strong></th><th><strong>最大输出长度</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>Qwen3-Max</strong></td><td><code>qwen3-max</code></td><td>256K</td><td>64K</td><td><strong>超大杯</strong>。非思考模式输出可达 64K，思考模式输出 32K。</td></tr><tr><td><strong>Qwen-Plus</strong></td><td><code>qwen-plus</code></td><td>1M</td><td>32K</td><td><strong>大杯</strong>。百万级长文本支持，适合复杂任务推理。</td></tr><tr><td><strong>Qwen-Flash</strong></td><td><code>qwen-flash</code></td><td>1M</td><td>32K</td><td><strong>中杯</strong>。兼顾百万级上下文与极速响应速度。</td></tr><tr><td><strong>Qwen3-VL-Plus</strong></td><td><code>qwen3-vl-plus</code></td><td>256K</td><td>32K</td><td><strong>视觉大杯</strong>。支持高分辨率，单图最大 16,384 tokens。</td></tr><tr><td><strong>Qwen3-VL-Flash</strong></td><td><code>qwen3-vl-flash</code></td><td>256K</td><td>32K</td><td><strong>视觉中杯</strong>。支持视觉推理模式，单图上限同 Plus。</td></tr><tr><td><strong>Qwen-Long</strong></td><td><code>qwen-long</code></td><td>10M</td><td>32K</td><td><strong>长文本专家</strong>。支持 1000 万 token 超长输入。</td></tr><tr><td><strong>Qwen3-Coder-Plus</strong></td><td><code>qwen3-coder-plus</code></td><td>1M</td><td>64K</td><td><strong>编码特化大杯</strong>。专为复杂编程设计，支持百万级上下文与 64K 超长输出。</td></tr><tr><td><strong>Qwen3-Coder-Flash</strong></td><td><code>qwen3-coder-flash</code></td><td>1M</td><td>64K</td><td><strong>编码特化小杯</strong>。高效处理编程任务，具备极高的响应速度。</td></tr></tbody></table><p>把目光转回到字节的豆包家族。阿里和字节基本上是截然相反的——字节在LLM上的开源很少，可用的只有<code>Seed-OSS-36B</code>，豆包底模（基座模型）也一直很一般。然而豆包的产品做的很好，在国内C端市占率遥遥领先。这当然得益于他们深耕多模态，但这可能和集团底色也有一定关系。如果你手机里需要一款不需要爬墙就很好用的AI应用，那我想应该是豆包没错了。但使用LLM API？除非你的公司疯狂迷恋Coze，好吧，这也是我对豆包不太感冒原因之一。</p><table><thead><tr><th><strong>模型名称</strong></th><th><strong>模型 ID</strong></th><th><strong>上下文长度</strong></th><th><strong>最大输出长度</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>Doubao-Seed-1.8</strong></td><td><code>doubao-seed-1-8-251215</code></td><td>256K</td><td>32K</td><td><strong>大杯</strong>。支持深度思考、多模态理解与工具调用，最长思维链达 64K。</td></tr><tr><td><strong>Doubao-Seed-Code</strong></td><td><code>doubao-seed-code-preview-251028</code></td><td>256K</td><td>32K</td><td><strong>编码特化</strong>。专为编程场景设计，支持深度思考与多模态理解。</td></tr><tr><td><strong>Doubao-Seed-Lite</strong></td><td><code>doubao-seed-1-6-lite-251015</code></td><td>256K</td><td>32K</td><td><strong>中杯</strong>。兼顾生成效率与推理能力，支持结构化输出。</td></tr><tr><td><strong>Doubao-Seed-Flash</strong></td><td><code>doubao-seed-1-6-flash-250828</code></td><td>256K</td><td>32K</td><td><strong>小杯</strong>。具备视觉定位能力，适用于高频多模态交互。</td></tr><tr><td><strong>Doubao-Seed-Vision</strong></td><td><code>doubao-seed-1-6-vision-250815</code></td><td>256K</td><td>32K</td><td><strong>视觉中杯（也可能是大杯？）</strong>。侧重 GUI 任务与复杂多模态理解。</td></tr></tbody></table><h1 id="开源模型"><a href="#开源模型" class="headerlink" title="开源模型"></a>开源模型</h1><p>开源模型从2025年年初到现在可谓是百花齐放、百家争鸣，锣鼓喧天，鞭炮齐鸣。</p><p>如果说商业闭源领域里美国是老大哥，那我们就是开源赛道上的扛把子。篇幅所限，本文只讨论2025至今热度最高的几家。当然，Meta的LLaMA也不再讨论，因为LLaMA 4很拉非常拉，只有LLaMa 3世代及其变体有一定的使用价值。</p><p>由于开源模型的参数及介绍均可以在Huggingface的Model Card及config.json中找到，这里的表格将不再赘述。</p><table><thead><tr><th>公司&#x2F;机构</th><th>AI 模型系列</th></tr></thead><tbody><tr><td>深度求索</td><td>DeepSeek系列</td></tr><tr><td>阿里巴巴</td><td>Qwen系列</td></tr><tr><td>智谱</td><td>GLM系列</td></tr><tr><td>月之暗面</td><td>Kimi系列</td></tr><tr><td>MiniMax</td><td>MiniMax系列</td></tr><tr><td>腾讯</td><td>Hunyuan（HY）系列</td></tr><tr><td>小米</td><td>Mimo系列</td></tr><tr><td>美团</td><td>LongCat系列</td></tr><tr><td>谷歌</td><td>Gemma系列</td></tr><tr><td>OpenAI</td><td>GPT-OSS系列</td></tr></tbody></table><h2 id="DeepSeek：真金不怕火炼"><a href="#DeepSeek：真金不怕火炼" class="headerlink" title="DeepSeek：真金不怕火炼"></a>DeepSeek：真金不怕火炼</h2><p>其实开源模型这里东西太多了，实在是写不过来。但DeepSeek实在太重要了，他的意义和实力非凡。</p><p>从最早的Dense模型，再到V2 &#x2F; V2.5转向MoE，最后到V3 &#x2F; R1的火爆出圈，深度求索一直在认真做事。早在V2.5时期，DeepSeek就以极致性价比吸引了我——那时候还没有那么多选择，中转站也是各种掺水或者价格高昂，心黑完了。</p><p>通义千问和DeepSeek有相似之处——拥抱开源，有真本事，产品稀碎。当然千问好歹还有几个带点C端功能的入口和web端一些让人眼前一亮同时非常实用的功能点，同时还有免费使用的超大杯夯到爆的Qwen Max，到了DeepSeek这就是纯纯的毛坯房，要啥没啥，只有最纯粹极致的便宜API，爱用不用。这非常败路人缘，比如某乎上随便什么人都可以出来批判一番。</p><p>DeepSeek V3 &#x2F; R1及其变体大规模应用了RL及合成数据训练，能力很强。但对齐在当下的环境来看是不太够用了，那么RL的苦果就不得不吞：幻觉严重，文风放飞自我到癫狂，创意写作如同梦呓，很难相信一个致幻剂违法的国家会诞生DeepSeek这种东西。</p><p>但正如同Claude日常被各种编码模型拉出来PK军训一样，DeepSeek也日常被各种模型拉出来当Base。无论你喜欢它还是讨厌它，它就在这。大杯模型守门员，开源领域试金石；只要在几个领域能够超越DeepSeek，就能加冕登基，打不过DeepSeek就别上桌吃饭了。</p><p>2025年1月份的V3 &#x2F; R1堪称LLM发癫史上最浓墨重彩的一笔。与同期的GPT &#x2F; Claude家族相比，幻觉几乎高了一个量级。意外地，很多人喜欢这一版，认为它富有想象力和文学性（虽然我感觉本质上这是胡扯的另外一种高情商表达形式）。但要命的是，由于DeepSeek被神秘力量大力宣传，不少尝试接触新技术的人被幻觉狠狠地坑了一把，从此路转黑。这一切发生的有点巧，有点可惜。后续版本渐渐补全了一些能力，然后在3.1上融合了思考与非思考（同期的Qwen3家族在吃了个亏以后反而把融合思考给拆开了），但又闹出来了个极你太美事件。</p><p>3.2上启用了稀疏注意力机制并且实现了交错思考，给2025年划上了一个句号。所有人都在期待着DeepSeek V4能否王者回归，再一次狠狠地踢闭源模型的屁股。</p><p>在当下的时间点上，DeepSeek属于万金油模型，样样都能做，样样都不精。但685B的参数配上非常美丽的价格，很适合作为企业接入的选项之一。</p><table><thead><tr><th>模型名称</th><th>备注</th></tr></thead><tbody><tr><td>DeepSeek-V3</td><td>初代V3，发癫</td></tr><tr><td>DeepSeek-V3-0324</td><td>解决了一部分发癫问题，但幻觉依旧</td></tr><tr><td>DeepSeek-V3.1</td><td>融合了思考与非思考，注意存在极你太美问题</td></tr><tr><td>DeepSeek-V3.1-Terminus</td><td>修复极你太美</td></tr><tr><td>DeepSeek-V3.2</td><td>目前最新的模型，靠DSA进一步压低推理价格</td></tr><tr><td>DeepSeek-R1</td><td>初代R1，发癫</td></tr><tr><td>DeepSeek-R1-0528</td><td>解决了一部分发癫问题，但幻觉依旧</td></tr></tbody></table><h2 id="阿里巴巴：乱拳打死老师傅"><a href="#阿里巴巴：乱拳打死老师傅" class="headerlink" title="阿里巴巴：乱拳打死老师傅"></a>阿里巴巴：乱拳打死老师傅</h2><p>被乱拳打死的是谁呢？好难猜，肯定不是Meta吧。阿里从Qwen 2.5开始彻底发力，和当时风头正盛的LLaMA分庭抗礼。当开源爱好者们都以为Meta要憋大招时，没想到拉了坨大的，LLaMA4和LMArena一起被扫进了历史的垃圾堆（同时酷爱刷LMA的还有文心ERNIE）。但把视线再向前推一下，Qwen2世代似乎就有搞模海战术的前兆。到了Qwen2.5开始则是给出了从适合学术研究的0.5B到能够执行绝大部分日常任务的72B各种尺寸，从CPU到3090都总有一款适合你。与此同时还带来了相当够用的Qwen2.5 VL系列，结束了国模无视觉大将的时代。</p><p>很可惜，当大家翘首期盼Qwen3的时候，不出意外应该是出意外了。Qwen3初代模型对齐也出现了问题，2504被钉上了耻辱柱。然而Qwen3家族的模型给的更多更全划分更细，从0.6B到235B，Dense和MoE都有，就算捏着鼻子也得品鉴。</p><p>很快，Qwen3 2507推出了，奠定了Qwen3家族真正的基础，现在使用的通义千问3家族基本都出自这一版。</p><p>Qwen系列出色的指令遵循能力以及开源各种权重及家族工具都使得Qwen成为了开发者不得不品鉴的一环。为微调（套皮）、开发AI应用或者企业私有化部署感到烦恼？看看Qwen的Huggingface仓库找找答案吧，你想要的基本都有。但端上来的太多力，求求你饶了我吧（</p><ul><li>如果有2507变体，则代指2507而非2504版本。</li><li>由于Qwen3家族的大语言模型实在太多，故拆分表格。</li><li>30B-A3B体感类似于14B Dense模型。</li><li>Qwen3 Next很好用，是中小企业部署的绝佳选择…如果不需要VL的话。</li></ul><p>Qwen3系列：</p><table><thead><tr><th>模型名</th><th>备注</th></tr></thead><tbody><tr><td><strong>Qwen3-235B-A22B-Thinking</strong></td><td>MoE模型，思考模式，专注推理能力（很耗Token！）</td></tr><tr><td><strong>Qwen3-235B-A22B-Instruct</strong></td><td>MoE模型，指令微调版本（同样有输出思维链的倾向）</td></tr><tr><td><strong>Qwen3-30B-A3B-Thinking</strong></td><td>MoE模型，思考模式。非常适合资源有限的用户推理，激活3B就算在核显上也能跑的很快</td></tr><tr><td><strong>Qwen3-30B-A3B-Instruct</strong></td><td>MoE模型，指令微调版本，同上</td></tr><tr><td><strong>Qwen3-32B</strong></td><td>密集模型，性能强于30BA3B，但Dense不适合端侧推理</td></tr><tr><td><strong>Qwen3-14B</strong></td><td>密集模型，从这里开始大部分家用显卡也能爽玩</td></tr><tr><td><strong>Qwen3-8B</strong></td><td>密集模型</td></tr><tr><td><strong>Qwen3-4B</strong></td><td>密集模型，从这里开始视作端侧推理范畴</td></tr><tr><td><strong>Qwen3-1.7B</strong></td><td>密集模型</td></tr><tr><td><strong>Qwen3-0.6B</strong></td><td>密集模型，学术研究用居多</td></tr></tbody></table><p>Qwen3-VL系列：</p><table><thead><tr><th>模型名</th><th>备注</th></tr></thead><tbody><tr><td><strong>Qwen3-VL-235B-A22B-Thinking</strong></td><td>MoE视觉语言模型，思考模式，开源视觉模的神</td></tr><tr><td><strong>Qwen3-VL-235B-A22B-Instruct</strong></td><td>MoE视觉语言模型，指令版本，同上</td></tr><tr><td><strong>Qwen3-VL-32B-Instruct</strong></td><td>很少用</td></tr><tr><td><strong>Qwen3-VL-30B-A3B-Instruct</strong></td><td>MoE视觉语言模型，推理飞快，也是神</td></tr><tr><td><strong>Qwen3-VL-8B-Instruct</strong></td><td>密集语言模型，实际上已经能胜任大部分任务</td></tr><tr><td><strong>Qwen3-VL-4B-Instruct</strong></td><td>很适合端侧推理</td></tr><tr><td><strong>Qwen3-VL-2B-Instruct</strong></td><td>有点小了，学术研究用居多</td></tr></tbody></table><p>Qwen3 Coder &amp; Next 系列：</p><table><thead><tr><th>模型名</th><th>备注</th></tr></thead><tbody><tr><td><strong>Qwen3-Coder-480B-A35B-Instruct</strong></td><td>480B参数MoE代码生成模型，够用，最重要的是Qwen Coder CLI提供不要钱的。但非常烧Token，已经不如更新的GLM和MiniMax好用了</td></tr><tr><td><strong>Qwen3-Coder-30B-A3B-Instruct</strong></td><td>国企&#x2F;军工码农优选，很容易就能本地部署起来</td></tr><tr><td><strong>Qwen3-Next-80B-A3B-Instruct</strong></td><td>门控注意力实现的稀疏模型，总参数80B保证了世界知识的前提上只要3B激活，非常好用</td></tr><tr><td><strong>Qwen3-Next-80B-A3B-Thinking</strong></td><td>同上，思考模式版本。本地部署再也不怕Qwen烧Token导致慢的抠脚啦，3B激活推理飞快</td></tr></tbody></table><h2 id="智谱、MiniMax、月之暗面"><a href="#智谱、MiniMax、月之暗面" class="headerlink" title="智谱、MiniMax、月之暗面"></a>智谱、MiniMax、月之暗面</h2><p>从六小龙到四小虎，其他的现在不管创新能力和作为消费者追求性价比来讲都逐渐的退出舞台了。</p><p>2025年里这三家真可谓是猛猛发力，想通了A÷的模式其实是对的，都转头去做Coding Agent模型了。但为什么每次宣传的时候都要把Claude拉出来遛…给人一种某米某花某ov发布的时候总要蹭一下苹果的感觉。其实产品已经做的挺好了，再接再厉就是。</p><p>先说智谱，前身是THUDM，属于从LLaMA时代一路走过来的老将了。当年本地部署最早能讲明白中文的就是<code>chatglm-6b</code>，给人留下了深刻印象，我也基于官方教程使用llama-factory微调过一个法律模型。可惜GLM时代一开始扭扭捏捏不肯开源，直到GLM4后期开始回过神来开始全面拥抱社区。GLM-4和GLM4-0414的牛刀小试给人足够好的印象，稳定低幻觉的输出非常适用于RAG场景。</p><p>在各家都间歇性拉坨大的大背景下，GLM憋了个好活，从4.5到4.7一路高歌猛进，用上了358B的MoE。当然这个尺寸基本上就跟HomeLab和小企业没缘分了，然而Coding Agent的能力突飞猛进也是事实。假如你写代码的时候不被允许使用国外大模型的话，GLM家族是非常好的选择，和MiniMax互为竞争对手。</p><table><thead><tr><th>模型名</th><th>备注</th></tr></thead><tbody><tr><td>GLM-4.5</td><td>358B-A32B，梦开始的地方</td></tr><tr><td>GLM-4.5-Air</td><td>GLM-4.5精简版，106B-A12B，非常适合中小企业部署使用，四张4090美美跑量化版</td></tr><tr><td>GLM-4.5V</td><td>带视觉的4.5 Air，但上下文长度受限</td></tr><tr><td>GLM-4.6</td><td>进一步增强了能力，相当够用</td></tr><tr><td>GLM-4.6V</td><td>其实就是带视觉的4.6 Air，只不过本代没出Air</td></tr><tr><td>GLM-4.7</td><td>最新的支持交错思考的旗舰型号</td></tr></tbody></table><p>然后是MiniMax，跟智谱真是一对苦命鸳鸯，在港股赛跑上市，最终智谱领先一天。MiniMax在学习Claude的路上更加激进，仅用了230B参数就能够和GLM系列掰手腕，处于编码模型的T1梯队。MiniMax-M2引入了交错思考，配合Agent方面的训练，在代码工程上的能力得到了显著提升。而且由于激活参数相比GLM系列更小，在推理速度上优势尤为显著。<strong>需要注意的是，在实际工作中one shot是很难的，很多时候都需要手动测试然后提出问题再尝试修复BUG才能得到最终满意的结果，所以推理速度是很重要的一个评判指标</strong>。</p><p>此外，MiniMax的音视频合成同样性价比很高，海外业务做的不错。</p><table><thead><tr><th>模型名</th><th>备注</th></tr></thead><tbody><tr><td>MiniMax-M2</td><td>230B-A10B</td></tr><tr><td>MiniMax-M2.1</td><td>进一步增强了代码能力</td></tr></tbody></table><p>最后是月之暗面的Kimi系列。月之暗面其实之前的底模都很一般（K1.5时代），但产品和营销都做的不错。可以说国产AI里月之暗面的产品能够跟豆包一较高下，而网络搜索效果可能还要更胜一筹。</p><p>有没有一种比DeepSeek更适合编码的DeepSeek？有的，那就是Kimi K2家族。K2在DeepSeek的架构上进行了Scaling up，用开源的1T巨模震撼了所有人的眼球。大参数量带来的泛化能力增强显而易见：情感更细腻，写代码也能够力大砖飞。后续K2趁热打铁推出了K2-Instruct-0905以进一步逼近Claude水平，还推出了带有思考模式的Kimi-K2-Thinking。够大的参数带来了更多的世界知识，不少冷门的编码场景K2也能够解决。优点说完了那缺点呢？1T巨模的计算成本实在是太高了，性价比和推理速度上是不如上面两家的。</p><p>Kimi是少有的网页比API好用的厂，网络搜索和深度研究都不错：</p><table><thead><tr><th>模型名</th><th>备注</th></tr></thead><tbody><tr><td>Kimi-K2-Instruct</td><td>1T巨模，不带思考</td></tr><tr><td>Kimi-K2-Instruct-0905</td><td>进一步增强了代码能力</td></tr><tr><td>Kimi-K2-Thinking</td><td>带思考的版本</td></tr></tbody></table><h2 id="腾讯、小米、美团：互联网厂赶晚集的救赎"><a href="#腾讯、小米、美团：互联网厂赶晚集的救赎" class="headerlink" title="腾讯、小米、美团：互联网厂赶晚集的救赎"></a>腾讯、小米、美团：互联网厂赶晚集的救赎</h2><p>看到其他厂做AI如火如荼，这几家也坐不住了。</p><p>腾讯和字节有点像的是，都在音视频媒体&#x2F;多模态方向发力。但腾讯的LLM…就此略过，也就<code>Hunyuan-A13B</code>有点价值，在80B这个档可以和Qwen掰手腕。混元的3D一直做的不错。</p><p>如果小米进场，那么这个时机一定已经成熟。小米在罗福莉加入后，MiMo家族也初露峥嵘。MiMo-7B及VL变体基本就是Qwen2.5上训练得来，没什么太多的可圈可点之处。但Miloco和MiMo-V2-Flash却收获了一些好评。Miloco是在MiMo-VL基础上再次训练得来的智能家居视觉模型，小尺寸保证了私有化部署的可行性，希望视觉大模型能够给智能家居再次注入一支强心针。而MiMo-V2-Flash是一款309B-A15B的MoE模型，前DeepSeek研究员携MiMo归来挑战老东家，V我50听复仇计划？目前MiMo V2处于免费阶段，也着重宣传了编码性能，但我更看希望看到的是MiMo在Agent结合米厂的各种智能硬件的落地探索，现在AI成功落地转化的场景太少了。</p><table><thead><tr><th>模型名</th><th>备注</th></tr></thead><tbody><tr><td>Xiaomi-MiMo-VL-Miloco-7B</td><td>视觉模型，私有化部署智能家居用</td></tr><tr><td>MiMo-V2-Flash</td><td>快速的编码&amp;Agent模型</td></tr></tbody></table><p>美团LongCat：龙猫？长猫？这个名字有点难绷。LongCat-Flash虽说叫Flash，但实际上是一款560B-A27B的大模。美团也在音视频上发力，除了大语言模型外还有生图模型以及视频模型，有点黏着Qwen贴身搏斗的感觉。只谈大模型来说，LongCat感觉平平无奇，且待后面发展。比起MiMo Flash的0.7元输入&#x2F;2.1元输出来说，LongCat-Flash的5元输出就显得有些不值了。</p><h2 id="谷歌、OpenAI：秀肌肉"><a href="#谷歌、OpenAI：秀肌肉" class="headerlink" title="谷歌、OpenAI：秀肌肉"></a>谷歌、OpenAI：秀肌肉</h2><p>俗话说，命里缺啥，名字里就得起啥，所以OpenAI不Open也非常合理。自从GPT-2后OpenAI就再也没放出LLM的权重，期间只是端出了Whisper。奥特曼在造了好几个月的势后，终于扭扭捏捏地放出了所谓堪比o3-mini的gpt-oss系列，可惜o3早已过气。而且gpt-oss真的能和o3-mini比肩吗？缺少了视觉能力、离谱的自我审查，还有着超高的幻觉，以至于LiveCodeBench上出现了gpt-oss-20b反杀gpt-oss-120b的奇观，因为120b版本的幻觉实在是太高了。</p><p>当然，事物总是有两面性的。gpt-oss的放出给了大家许多启发，比如稀疏注意力、原生MXFP4、在思考中调用工具…总之，这是一个适合研究学习用的模型，而不太适合中国应用场景下的部署使用。</p><table><thead><tr><th>模型名</th><th>备注</th></tr></thead><tbody><tr><td>gpt-oss-20b</td><td>原生MXFP4，很小很快，处理日常任务还挺好用</td></tr><tr><td>gpt-oss-120b</td><td>一坨，强烈不推荐。在Antigravity里和Claude 4.5 Opus并列有一种我和科比合砍83分的美感</td></tr></tbody></table><p>到了谷歌这里又不太一样了：Gemma家族在国外社区还是一个不错的选项。Gemma 3虽然已经较老，但27B Dense结构带视觉在某些特定场景下可能也会有不错的效果，与Qwen3-VL可以掰掰手腕。尤其需要注意的是，Gemma 3有许多变体，比如医疗领域微调的medgemma、端侧使用的t5gemma、gemma-3n等。这里仅介绍主力Gemma 3家族，对端侧推理感兴趣的可以自行了解。</p><p>Gemma 3比起Qwen 3家族的优势之一是谷歌给出了Gemma 3 QAT权重。量化感知训练版本比起传统PTQ来说，可以在量化到更低精度时保持较好的性能。这对于HomeLab玩家来说是一个好消息。如果你希望在家里部署一个性能尚可且带有视觉的大模型，你需要做出选择的就是Gemma 3还是Qwen 3。</p><p>下方是原始指令对齐权重（其他变体请在Huggingface上自行查询）：</p><table><thead><tr><th>模型名</th><th>备注</th></tr></thead><tbody><tr><td>gemma-3-1b-it</td><td>-</td></tr><tr><td>gemma-3-4b-it</td><td>-</td></tr><tr><td>gemma-3-12b-it</td><td>-</td></tr><tr><td>gemma-3-27b-it</td><td>-</td></tr></tbody></table></div><hr><div><div class="post-metas my-3"><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/LLM/">#LLM</a> <a href="/tags/VLM/">#VLM</a></div></div><div class="license-box my-3"><div class="license-title"><div>各大常用模型特点</div><div>https://github.com/yangxiangnanwill/yangxiangnanwill.github.io/2026/01/05/好好码代码吖/LLM/大模型简介/各大常用模型特点/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>will</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2026年1月5日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"></article><article class="post-next col-6"><a href="/2026/01/04/%E5%A5%BD%E5%A5%BD%E7%A0%81%E4%BB%A3%E7%A0%81%E5%90%96/AI/ClaudeCode%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93/" title="ClaudeCode实践总结"><span class="hidden-mobile">ClaudeCode实践总结</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing;(t=t.getElementById("subtitle"))&&e&&e(t.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){var t;"tocbot"in window&&(tocbot.refresh(),0!==(t=jQuery("#toc")).length)&&tocbot&&0<t.find(".toc-list-item").length&&t.css("visibility","visible")}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback(function(){if("anchors"in window){anchors.removeAll();var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}})})</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>