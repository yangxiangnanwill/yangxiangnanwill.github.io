{
    "version": "https://jsonfeed.org/version/1",
    "title": "Will • All posts by \"大模型简介\" category",
    "description": "愿你一生努力，一生被爱",
    "home_page_url": "https://github.com/yangxiangnanwill/yangxiangnanwill.github.io",
    "items": [
        {
            "id": "https://github.com/yangxiangnanwill/yangxiangnanwill.github.io/2026/01/05/%E5%A5%BD%E5%A5%BD%E7%A0%81%E4%BB%A3%E7%A0%81%E5%90%96/LLM/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B/%E5%90%84%E5%A4%A7%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9/",
            "url": "https://github.com/yangxiangnanwill/yangxiangnanwill.github.io/2026/01/05/%E5%A5%BD%E5%A5%BD%E7%A0%81%E4%BB%A3%E7%A0%81%E5%90%96/LLM/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B/%E5%90%84%E5%A4%A7%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9/",
            "title": "各大常用模型特点",
            "date_published": "2026-01-05T02:30:52.535Z",
            "content_html": "<blockquote>\n<p>本章以开源&#x2F;闭源模型为划分，介绍一下日常使用及评估的经验。可能较为主观，当然一些介绍就直接照搬了，这里不做过多的赘述，请各位看官也要多多结合自身体感及实际业务体验来评判。</p>\n</blockquote>\n<h1 id=\"闭源模型\"><a href=\"#闭源模型\" class=\"headerlink\" title=\"闭源模型\"></a>闭源模型</h1><p><strong>闭源模型：一种循环</strong></p>\n<p>目前实现了SOTA（<strong>State</strong> <strong>of</strong> <strong>the</strong> <strong>Art</strong>，特定领域或任务中，当前的最新进展和最高水准，基本上是各家自称）的闭源模型厂主要有如下几家（豆包除外，稍后单讲）：</p>\n<table>\n<thead>\n<tr>\n<th>公司&#x2F;机构</th>\n<th>AI 模型系列</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>OpenAI</td>\n<td>GPT系列</td>\n</tr>\n<tr>\n<td>Google</td>\n<td>Google Gemini系列</td>\n</tr>\n<tr>\n<td>Anthropic</td>\n<td>Claude系列</td>\n</tr>\n<tr>\n<td>xAI</td>\n<td>Grok系列</td>\n</tr>\n<tr>\n<td>阿里巴巴</td>\n<td>通义千问系列</td>\n</tr>\n<tr>\n<td>字节跳动</td>\n<td>豆包系列</td>\n</tr>\n</tbody></table>\n<p>这几家基本上每隔一段时间就宣称自己发布了最强大的xx模型，以至于形成了一种循环。当然SOTA这个词很微妙，最新最大杯的模型未必就最适合你。下面按照模型家族介绍一下本代的各种主力型号的特点：</p>\n<h2 id=\"OpenAI-GPT：冷静的理性思考\"><a href=\"#OpenAI-GPT：冷静的理性思考\" class=\"headerlink\" title=\"OpenAI GPT：冷静的理性思考\"></a>OpenAI GPT：冷静的理性思考</h2><p>自从迈入GPT-5时代以来，GPT系列模型就以回复简短闻名。从好的方面看，OpenAI做到了省output token（输出token数），这使得任务总体所需时间进一步得到压缩。然而代价是冷漠到近乎不近人情的回复使得创意写作用户不得不忍痛抛弃它。后续推出的编码特化模型<code>gpt5-codex</code>模型进一步强化了这个特征，有时候描述性文字几乎已经不能称之为人话了。好在GPT-5.2系列在一定程度上解决了这个问题，虽然比起GPT-4.5甚至GPT-4o系列模型给人在Chat上的主观感受仍有差距，但已经较为可用。</p>\n<p>OpenAI作为LLM的领头羊，服务器的压力自然是很大的，无论是网页还是API都可能会有服务异常的情况。为了解决这个问题，GPT-5系列在网页端给出的解决方案是自动路由（其实就是超级降智）。然而，对于指定了特定型号的API用户来说，GPT-5系列模型的推理速度仍然显得相对较慢。</p>\n<p>说完了缺点，那么剩下的基本上全是优点。回复简短意味着完成同等任务下所需tokens更少，冷静的理性思考带给人一种指哪打哪的感觉——不废话，just do it。比起GPT-4时代的人味儿来说，GPT-5更像一名理工男。当然，它是一名后端理工男，在审美上未必有多好的品味。</p>\n<table>\n<thead>\n<tr>\n<th>模型名称</th>\n<th>模型 ID</th>\n<th>上下文长度</th>\n<th>最大输出长度</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>GPT-5.2 Thinking</strong></td>\n<td><code>gpt-5.2</code>（<code>gpt-5.2-2025-12-11</code>）</td>\n<td>400K</td>\n<td>128K</td>\n<td>最高推理强度，支持 reasoning 参数（大杯）</td>\n</tr>\n<tr>\n<td><strong>GPT-5.2 Pro</strong></td>\n<td><code>gpt-5.2-pro</code></td>\n<td>400K</td>\n<td>128K</td>\n<td>企业级最高准确度，支持 xhigh reasoning（超大杯）</td>\n</tr>\n<tr>\n<td><strong>GPT-5.2 Chat (Instant)</strong></td>\n<td><code>gpt-5.2-chat-latest</code></td>\n<td>128K</td>\n<td>16K</td>\n<td>ChatGPT“GPT-5.2 即时”模式，延迟最低（其实就是小杯，很蠢）</td>\n</tr>\n<tr>\n<td><strong>GPT-5.2 (base)</strong></td>\n<td><code>gpt-5.2</code></td>\n<td>400K</td>\n<td>128K</td>\n<td>通用旗舰版，默认 reasoning&#x3D;medium（中杯）</td>\n</tr>\n<tr>\n<td><strong>GPT-5.2-Codex</strong></td>\n<td><code>gpt-5.2-codex</code></td>\n<td>400K</td>\n<td>128K</td>\n<td>代理式编码专用，支持上下文压缩与视觉输入</td>\n</tr>\n<tr>\n<td><strong>GPT-5.1-Codex-Max</strong></td>\n<td><code>gpt-5.1-codex-max</code></td>\n<td>400K</td>\n<td>128K</td>\n<td>支持“压缩”技术，可跨多窗口连贯处理数百万 tokens，专为长时间、项目级编码任务设计</td>\n</tr>\n</tbody></table>\n<p>这里需要特别注意的是，<code>gpt-5.2-codex</code>并非代码万灵药。如果你不太会写prompt或者这个工程需要范围更广的探索思考，那么<code>gpt-5.2</code>可能会比codex变体好用些。codex更突出指哪打哪的能力，而<code>gpt-5.2</code>会主动帮你多想些。换句话说，改bug用<code>gpt-5.2-codex</code>，新开工程&#x2F;模块用<code>gpt-5.2</code>。推荐写后端或复杂的前端逻辑时使用GPT系列模型。</p>\n<h2 id=\"Gemini：多模态和世界知识之王\"><a href=\"#Gemini：多模态和世界知识之王\" class=\"headerlink\" title=\"Gemini：多模态和世界知识之王\"></a>Gemini：多模态和世界知识之王</h2><p>老谷坐拥无尽的网络资源宝库以及Deepmind+TPU的神秘力量加持，尽管在LLM时代赶了个晚集，但从Gemini 2.0开始一路猛追，到了2.5时代已经是妥妥的御三家之一。Gemini的多模态能力令人惊叹，Pro系列的世界知识更是让人折服。比起GPT来说，Gemini更像一名文科生：大参数带来的丰富世界知识给了它更强的文学理解能力，思考之细腻和情感共鸣能力使得它成为创意写作的最优选。当接入Chatbot的时候，你甚至可能没法分清它到底是AI还是人——太能接梗了。</p>\n<p>大家都不知道Gemini Pro系列的参数到底有多大，目前普遍认为1T以上。然而推理速度比起其他各家大参数模型来说又快的离谱，疑似Jeff Dean在机房里手敲（其实应该是TPU的特点所致）。总之，如果你想选择一款有超强的世界知识并且对推理速度有一定要求的模型，那么Gemini系列是毋庸置疑的选择。</p>\n<p>Gemini 3.0 Pro从内部测试阶段就不断炸场，多模态+大参数写出的前端效果惊艳了所有关注AI前沿动向的人。尽管Gemini 3.0 Pro存在较为严重的长上下文幻觉问题，但瑕不掩瑜，它依然是现在最适合前端的模型。</p>\n<p>Gemini 3.0 Flash推出后，甚至神秘地实现了某种程度上对Pro的反杀，几乎和Pro一样丰富的世界知识和更好的编码能力。下克上？搞不懂老谷。</p>\n<table>\n<thead>\n<tr>\n<th><strong>模型名称</strong></th>\n<th><strong>模型 ID</strong></th>\n<th><strong>上下文长度</strong></th>\n<th><strong>最大输出长度</strong></th>\n<th><strong>备注</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Gemini 3 Pro</strong></td>\n<td><code>gemini-3-pro</code></td>\n<td>1000K (1M)</td>\n<td>64K</td>\n<td><strong>旗舰模型</strong>。最强多模态推理与编码能力，支持 <code>high</code> 深度思维模式。前端很强非常强！但受限于长上下文幻觉，后端稀烂（相比其他两家）</td>\n</tr>\n<tr>\n<td><strong>Gemini 3 Flash</strong></td>\n<td><code>gemini-3-flash</code></td>\n<td>1000K (1M)</td>\n<td>64K</td>\n<td><strong>速度旗舰</strong>。专为 Agent 设计，支持 <code>minimal/medium</code> 等多级思维调节。Flash反杀Pro！大部分搬砖的活计用Flash就够了，速度飞快。</td>\n</tr>\n<tr>\n<td><strong>Gemini 2.5 Pro</strong></td>\n<td><code>gemini-2.5-pro</code></td>\n<td>1000K (1M)</td>\n<td>64K</td>\n<td>2.5 世代旗舰。具备极强的长文本召回能力。（前面是官方说法，实际上各家长文本都一坨）</td>\n</tr>\n<tr>\n<td><strong>Gemini 2.5 Flash</strong></td>\n<td><code>gemini-2.5-flash</code></td>\n<td>1000K (1M)</td>\n<td>64K</td>\n<td>2.5 世代均衡版。高吞吐量，默认支持长上下文处理。</td>\n</tr>\n<tr>\n<td><strong>Gemini 2.5 Flash-Lite</strong></td>\n<td><code>gemini-2.5-flash-lite</code></td>\n<td>1000K (1M)</td>\n<td>64K</td>\n<td><strong>极致性价比</strong>。针对极低延迟任务优化，是目前最廉价的百万上下文模型。</td>\n</tr>\n</tbody></table>\n<h2 id=\"Anthropic-Claude：最均衡的编码代理模型\"><a href=\"#Anthropic-Claude：最均衡的编码代理模型\" class=\"headerlink\" title=\"Anthropic Claude：最均衡的编码代理模型\"></a>Anthropic Claude：最均衡的编码代理模型</h2><p>Anthropic，又称A÷ &#x2F; A畜，大家很熟悉了，神一样的Coding Agent，翔一样的口碑和服务可用性。抛开立场不谈，最早的Claude模型以创意写作闻名，比起同期的GPT-3.5来说回答更有人味。后来Claude率先扩展了长上下文窗口以及STEM能力，走向了编码特化的不归路。到了Claude 3时代开始就是彻头彻尾的Coding模型了，直到现在的Claude 4.5成为了最均衡的编码代理模型——如果你想前后端一把抓，选它准没错。强大的规划能力能够给出更适合工程上的方案，在各种场景下都能很好的完成目标。跑分没赢过，体验没输过。尽管日常处于即将被超越的状态，但还没被超越不是吗？（对标苹果，友商是xx！）</p>\n<table>\n<thead>\n<tr>\n<th><strong>模型名称</strong></th>\n<th><strong>模型 ID</strong></th>\n<th><strong>上下文长度</strong></th>\n<th><strong>最大输出长度</strong></th>\n<th><strong>备注</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Claude 4.5 Opus</strong></td>\n<td><code>claude-4-5-opus-20251124</code></td>\n<td>200K</td>\n<td>64K</td>\n<td>支持 <code>effort</code> 参数调节推理强度。编码与科研任务首选（超大杯）（反重力反代优选）</td>\n</tr>\n<tr>\n<td><strong>Claude 4.5 Sonnet</strong></td>\n<td><code>claude-4-5-sonnet-20250929</code></td>\n<td>200K &#x2F; 1000K*</td>\n<td>64K</td>\n<td>专为复杂 Agent 与项目级代码设计，性能超越早期 Opus 4（中杯）（对于反重力用户来说，有Opus谁用Sonnet）</td>\n</tr>\n<tr>\n<td><strong>Claude 4.5 Haiku</strong></td>\n<td><code>claude-4-5-haiku-20251014</code></td>\n<td>200K</td>\n<td>64K</td>\n<td>路边一条，官方说具备 Sonnet 4 级别的性能，但被Gemini Flash家族打出shi来了</td>\n</tr>\n</tbody></table>\n<p>注：只有官方Max订阅才有1000K上下文，大部分渠道都是200K的上下文，比如反重力逆向或Kiro逆向。</p>\n<h2 id=\"xAI-Grok：力大砖飞，以及瑟瑟\"><a href=\"#xAI-Grok：力大砖飞，以及瑟瑟\" class=\"headerlink\" title=\"xAI Grok：力大砖飞，以及瑟瑟\"></a>xAI Grok：力大砖飞，以及瑟瑟</h2><p>马斯克也许缺乏品味，但他足够有钱。Grok好不好用先放一边，超大规模的显卡集群是实打实存在的。这个系列一直秉持力大砖飞的原则，猛堆参数。迫于Scaling law的存在，就算是几百头猪，炼进Transformer里也能出些成果了罢。</p>\n<p>Grok在某些领域有着和Gemini系列相似的特性：参数够大，很适合创意写作任务。Grok 4家族拥有不俗的吐槽能力，在对齐上比起<code>a helpful assistant</code>来说更像一名沙雕网友。而且Grok背靠X（aka Twitter），也有着丰富的语料及不错的搜索功能。对于老外来说，Grok简直是全自动开盒器（is that true ? )</p>\n<p>Grok系列另一个令人津津乐道的地方就是极低的审查下限。在各家API中，Grok &#x2F; Google Vertex &#x2F; DeepSeek是审查力度相对较低的。但到了网页端上Grok也保持极低的审查下限就很离谱，当然考虑到X网页端上你依然可以畅爽NSFW…好吧，Grok适合搞瑟瑟是从娘胎里就带出来的本事。无需破甲，无需诱导，很黄很暴力。酒馆和各种文字扮演游戏的常客。</p>\n<table>\n<thead>\n<tr>\n<th><strong>模型名称</strong></th>\n<th><strong>模型 ID</strong></th>\n<th><strong>上下文长度</strong></th>\n<th><strong>最大输出长度</strong></th>\n<th><strong>备注</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Grok 4 Heavy (SuperGrok)</strong></td>\n<td><code>grok-4-heavy</code></td>\n<td>256K</td>\n<td>8K - 16K</td>\n<td><strong>多智能体协作系统</strong>，通过并行推理验证结果，推理强度最高（超大杯）</td>\n</tr>\n<tr>\n<td><strong>Grok 4.1</strong></td>\n<td><code>grok-4.1</code></td>\n<td>256K</td>\n<td>16K</td>\n<td>2025年底旗舰，主打<strong>高情商 (EQ)</strong> 与低幻觉率，创意写作能力很好（大杯）</td>\n</tr>\n<tr>\n<td><strong>Grok 4</strong></td>\n<td><code>grok-4</code></td>\n<td>256K</td>\n<td>8K</td>\n<td>2025年中发布的标准旗舰，原生支持多模态推理与实时 X 搜索</td>\n</tr>\n<tr>\n<td><strong>Grok 4.1 Fast (Long)</strong></td>\n<td><code>grok-4.1-fast</code></td>\n<td>2,000K</td>\n<td>16K</td>\n<td><strong>超长上下文版</strong>，支持 200 万 token，类似Gemini Flash（中杯）</td>\n</tr>\n<tr>\n<td><strong>Grok 4 Fast (Instant)</strong></td>\n<td><code>grok-4-fast</code></td>\n<td>2,000K</td>\n<td>30K</td>\n<td><strong>极速&#x2F;高性价比版</strong>，支持 reasoning 切换（可关闭推理以获得极低延迟，类似Gemini Flash Lite，小杯）</td>\n</tr>\n<tr>\n<td><strong>Grok Code Fast 1</strong></td>\n<td><code>grok-code-fast-1</code></td>\n<td>256K</td>\n<td>16K</td>\n<td><strong>马斯克的钞能力</strong>，在一众编程模型当中显得平平无奇，但不要钱不要钱不要钱！速度很快，质量一般，体感跟Gemini 2.5 Flash差不多的性能，但在各种 Vibe Coding 客户端里都作为免费选项出现。</td>\n</tr>\n</tbody></table>\n<h2 id=\"阿里-通义千问-amp-字节跳动-豆包\"><a href=\"#阿里-通义千问-amp-字节跳动-豆包\" class=\"headerlink\" title=\"阿里 通义千问 &amp; 字节跳动 豆包\"></a>阿里 通义千问 &amp; 字节跳动 豆包</h2><blockquote>\n<p>能力先行还是产品先行？</p>\n</blockquote>\n<p>阿里作为目前开源界当之无愧的扛把子，从Meta手中接过了开源的大旗。r&#x2F;LocalLlama如今已是r&#x2F;LocalQwen的形状了。Qwen家族分为开源模型和闭源模型两种。除了每代的超大杯（通义千问Max）为闭源外，其他商业API均能找到对应的类似开源型号。通义千问的特点是极强的指令遵循能力和稀烂的产品。</p>\n<p>Qwen家族的模型在输出上总感觉缺了点味道。它不像GPT那样冷静简洁，不像Gemini那样细腻有人味，但也不像DeepSeek R1 0120那样放飞自我。很怪，AI味很重，在大规模使用RL训练的Qwen3世代这个特点尤为显著。国模的通病之一在Qwen上有显著体现：思考时非常消耗Token，甚至在Instruct模型上模型倾向于输出思维链，导致最终完成复杂任务时所耗Token相对较高。</p>\n<p>但从另一个方面上来讲，Qwen作为国内AI的T0选手，其模型非常适合国内企业落地开发使用：性价比适中、模型选择丰富、较好的服务稳定性，还有强大的指令遵循能力可以减轻不少开发难度。逻辑能力也相当不错。</p>\n<p>阿里系除了主打的阿里云百炼平台提供的通义千问服务外，还有面向开发者的modelscope（魔搭）、心流团队的iFlow、面向C端的蚂蚁灵光系列，主打一个养蛊和乱拳打死老师傅。以下表格主要介绍闭源的通义千问3家族：</p>\n<table>\n<thead>\n<tr>\n<th><strong>模型名称</strong></th>\n<th><strong>模型 ID</strong></th>\n<th><strong>上下文长度</strong></th>\n<th><strong>最大输出长度</strong></th>\n<th><strong>备注</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Qwen3-Max</strong></td>\n<td><code>qwen3-max</code></td>\n<td>256K</td>\n<td>64K</td>\n<td><strong>超大杯</strong>。非思考模式输出可达 64K，思考模式输出 32K。</td>\n</tr>\n<tr>\n<td><strong>Qwen-Plus</strong></td>\n<td><code>qwen-plus</code></td>\n<td>1M</td>\n<td>32K</td>\n<td><strong>大杯</strong>。百万级长文本支持，适合复杂任务推理。</td>\n</tr>\n<tr>\n<td><strong>Qwen-Flash</strong></td>\n<td><code>qwen-flash</code></td>\n<td>1M</td>\n<td>32K</td>\n<td><strong>中杯</strong>。兼顾百万级上下文与极速响应速度。</td>\n</tr>\n<tr>\n<td><strong>Qwen3-VL-Plus</strong></td>\n<td><code>qwen3-vl-plus</code></td>\n<td>256K</td>\n<td>32K</td>\n<td><strong>视觉大杯</strong>。支持高分辨率，单图最大 16,384 tokens。</td>\n</tr>\n<tr>\n<td><strong>Qwen3-VL-Flash</strong></td>\n<td><code>qwen3-vl-flash</code></td>\n<td>256K</td>\n<td>32K</td>\n<td><strong>视觉中杯</strong>。支持视觉推理模式，单图上限同 Plus。</td>\n</tr>\n<tr>\n<td><strong>Qwen-Long</strong></td>\n<td><code>qwen-long</code></td>\n<td>10M</td>\n<td>32K</td>\n<td><strong>长文本专家</strong>。支持 1000 万 token 超长输入。</td>\n</tr>\n<tr>\n<td><strong>Qwen3-Coder-Plus</strong></td>\n<td><code>qwen3-coder-plus</code></td>\n<td>1M</td>\n<td>64K</td>\n<td><strong>编码特化大杯</strong>。专为复杂编程设计，支持百万级上下文与 64K 超长输出。</td>\n</tr>\n<tr>\n<td><strong>Qwen3-Coder-Flash</strong></td>\n<td><code>qwen3-coder-flash</code></td>\n<td>1M</td>\n<td>64K</td>\n<td><strong>编码特化小杯</strong>。高效处理编程任务，具备极高的响应速度。</td>\n</tr>\n</tbody></table>\n<p>把目光转回到字节的豆包家族。阿里和字节基本上是截然相反的——字节在LLM上的开源很少，可用的只有<code>Seed-OSS-36B</code>，豆包底模（基座模型）也一直很一般。然而豆包的产品做的很好，在国内C端市占率遥遥领先。这当然得益于他们深耕多模态，但这可能和集团底色也有一定关系。如果你手机里需要一款不需要爬墙就很好用的AI应用，那我想应该是豆包没错了。但使用LLM API？除非你的公司疯狂迷恋Coze，好吧，这也是我对豆包不太感冒原因之一。</p>\n<table>\n<thead>\n<tr>\n<th><strong>模型名称</strong></th>\n<th><strong>模型 ID</strong></th>\n<th><strong>上下文长度</strong></th>\n<th><strong>最大输出长度</strong></th>\n<th><strong>备注</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Doubao-Seed-1.8</strong></td>\n<td><code>doubao-seed-1-8-251215</code></td>\n<td>256K</td>\n<td>32K</td>\n<td><strong>大杯</strong>。支持深度思考、多模态理解与工具调用，最长思维链达 64K。</td>\n</tr>\n<tr>\n<td><strong>Doubao-Seed-Code</strong></td>\n<td><code>doubao-seed-code-preview-251028</code></td>\n<td>256K</td>\n<td>32K</td>\n<td><strong>编码特化</strong>。专为编程场景设计，支持深度思考与多模态理解。</td>\n</tr>\n<tr>\n<td><strong>Doubao-Seed-Lite</strong></td>\n<td><code>doubao-seed-1-6-lite-251015</code></td>\n<td>256K</td>\n<td>32K</td>\n<td><strong>中杯</strong>。兼顾生成效率与推理能力，支持结构化输出。</td>\n</tr>\n<tr>\n<td><strong>Doubao-Seed-Flash</strong></td>\n<td><code>doubao-seed-1-6-flash-250828</code></td>\n<td>256K</td>\n<td>32K</td>\n<td><strong>小杯</strong>。具备视觉定位能力，适用于高频多模态交互。</td>\n</tr>\n<tr>\n<td><strong>Doubao-Seed-Vision</strong></td>\n<td><code>doubao-seed-1-6-vision-250815</code></td>\n<td>256K</td>\n<td>32K</td>\n<td><strong>视觉中杯（也可能是大杯？）</strong>。侧重 GUI 任务与复杂多模态理解。</td>\n</tr>\n</tbody></table>\n<h1 id=\"开源模型\"><a href=\"#开源模型\" class=\"headerlink\" title=\"开源模型\"></a>开源模型</h1><p>开源模型从2025年年初到现在可谓是百花齐放、百家争鸣，锣鼓喧天，鞭炮齐鸣。</p>\n<p>如果说商业闭源领域里美国是老大哥，那我们就是开源赛道上的扛把子。篇幅所限，本文只讨论2025至今热度最高的几家。当然，Meta的LLaMA也不再讨论，因为LLaMA 4很拉非常拉，只有LLaMa 3世代及其变体有一定的使用价值。</p>\n<p>由于开源模型的参数及介绍均可以在Huggingface的Model Card及config.json中找到，这里的表格将不再赘述。</p>\n<table>\n<thead>\n<tr>\n<th>公司&#x2F;机构</th>\n<th>AI 模型系列</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>深度求索</td>\n<td>DeepSeek系列</td>\n</tr>\n<tr>\n<td>阿里巴巴</td>\n<td>Qwen系列</td>\n</tr>\n<tr>\n<td>智谱</td>\n<td>GLM系列</td>\n</tr>\n<tr>\n<td>月之暗面</td>\n<td>Kimi系列</td>\n</tr>\n<tr>\n<td>MiniMax</td>\n<td>MiniMax系列</td>\n</tr>\n<tr>\n<td>腾讯</td>\n<td>Hunyuan（HY）系列</td>\n</tr>\n<tr>\n<td>小米</td>\n<td>Mimo系列</td>\n</tr>\n<tr>\n<td>美团</td>\n<td>LongCat系列</td>\n</tr>\n<tr>\n<td>谷歌</td>\n<td>Gemma系列</td>\n</tr>\n<tr>\n<td>OpenAI</td>\n<td>GPT-OSS系列</td>\n</tr>\n</tbody></table>\n<h2 id=\"DeepSeek：真金不怕火炼\"><a href=\"#DeepSeek：真金不怕火炼\" class=\"headerlink\" title=\"DeepSeek：真金不怕火炼\"></a>DeepSeek：真金不怕火炼</h2><p>其实开源模型这里东西太多了，实在是写不过来。但DeepSeek实在太重要了，他的意义和实力非凡。</p>\n<p>从最早的Dense模型，再到V2 &#x2F; V2.5转向MoE，最后到V3 &#x2F; R1的火爆出圈，深度求索一直在认真做事。早在V2.5时期，DeepSeek就以极致性价比吸引了我——那时候还没有那么多选择，中转站也是各种掺水或者价格高昂，心黑完了。</p>\n<p>通义千问和DeepSeek有相似之处——拥抱开源，有真本事，产品稀碎。当然千问好歹还有几个带点C端功能的入口和web端一些让人眼前一亮同时非常实用的功能点，同时还有免费使用的超大杯夯到爆的Qwen Max，到了DeepSeek这就是纯纯的毛坯房，要啥没啥，只有最纯粹极致的便宜API，爱用不用。这非常败路人缘，比如某乎上随便什么人都可以出来批判一番。</p>\n<p>DeepSeek V3 &#x2F; R1及其变体大规模应用了RL及合成数据训练，能力很强。但对齐在当下的环境来看是不太够用了，那么RL的苦果就不得不吞：幻觉严重，文风放飞自我到癫狂，创意写作如同梦呓，很难相信一个致幻剂违法的国家会诞生DeepSeek这种东西。</p>\n<p>但正如同Claude日常被各种编码模型拉出来PK军训一样，DeepSeek也日常被各种模型拉出来当Base。无论你喜欢它还是讨厌它，它就在这。大杯模型守门员，开源领域试金石；只要在几个领域能够超越DeepSeek，就能加冕登基，打不过DeepSeek就别上桌吃饭了。</p>\n<p>2025年1月份的V3 &#x2F; R1堪称LLM发癫史上最浓墨重彩的一笔。与同期的GPT &#x2F; Claude家族相比，幻觉几乎高了一个量级。意外地，很多人喜欢这一版，认为它富有想象力和文学性（虽然我感觉本质上这是胡扯的另外一种高情商表达形式）。但要命的是，由于DeepSeek被神秘力量大力宣传，不少尝试接触新技术的人被幻觉狠狠地坑了一把，从此路转黑。这一切发生的有点巧，有点可惜。后续版本渐渐补全了一些能力，然后在3.1上融合了思考与非思考（同期的Qwen3家族在吃了个亏以后反而把融合思考给拆开了），但又闹出来了个极你太美事件。</p>\n<p>3.2上启用了稀疏注意力机制并且实现了交错思考，给2025年划上了一个句号。所有人都在期待着DeepSeek V4能否王者回归，再一次狠狠地踢闭源模型的屁股。</p>\n<p>在当下的时间点上，DeepSeek属于万金油模型，样样都能做，样样都不精。但685B的参数配上非常美丽的价格，很适合作为企业接入的选项之一。</p>\n<table>\n<thead>\n<tr>\n<th>模型名称</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>DeepSeek-V3</td>\n<td>初代V3，发癫</td>\n</tr>\n<tr>\n<td>DeepSeek-V3-0324</td>\n<td>解决了一部分发癫问题，但幻觉依旧</td>\n</tr>\n<tr>\n<td>DeepSeek-V3.1</td>\n<td>融合了思考与非思考，注意存在极你太美问题</td>\n</tr>\n<tr>\n<td>DeepSeek-V3.1-Terminus</td>\n<td>修复极你太美</td>\n</tr>\n<tr>\n<td>DeepSeek-V3.2</td>\n<td>目前最新的模型，靠DSA进一步压低推理价格</td>\n</tr>\n<tr>\n<td>DeepSeek-R1</td>\n<td>初代R1，发癫</td>\n</tr>\n<tr>\n<td>DeepSeek-R1-0528</td>\n<td>解决了一部分发癫问题，但幻觉依旧</td>\n</tr>\n</tbody></table>\n<h2 id=\"阿里巴巴：乱拳打死老师傅\"><a href=\"#阿里巴巴：乱拳打死老师傅\" class=\"headerlink\" title=\"阿里巴巴：乱拳打死老师傅\"></a>阿里巴巴：乱拳打死老师傅</h2><p>被乱拳打死的是谁呢？好难猜，肯定不是Meta吧。阿里从Qwen 2.5开始彻底发力，和当时风头正盛的LLaMA分庭抗礼。当开源爱好者们都以为Meta要憋大招时，没想到拉了坨大的，LLaMA4和LMArena一起被扫进了历史的垃圾堆（同时酷爱刷LMA的还有文心ERNIE）。但把视线再向前推一下，Qwen2世代似乎就有搞模海战术的前兆。到了Qwen2.5开始则是给出了从适合学术研究的0.5B到能够执行绝大部分日常任务的72B各种尺寸，从CPU到3090都总有一款适合你。与此同时还带来了相当够用的Qwen2.5 VL系列，结束了国模无视觉大将的时代。</p>\n<p>很可惜，当大家翘首期盼Qwen3的时候，不出意外应该是出意外了。Qwen3初代模型对齐也出现了问题，2504被钉上了耻辱柱。然而Qwen3家族的模型给的更多更全划分更细，从0.6B到235B，Dense和MoE都有，就算捏着鼻子也得品鉴。</p>\n<p>很快，Qwen3 2507推出了，奠定了Qwen3家族真正的基础，现在使用的通义千问3家族基本都出自这一版。</p>\n<p>Qwen系列出色的指令遵循能力以及开源各种权重及家族工具都使得Qwen成为了开发者不得不品鉴的一环。为微调（套皮）、开发AI应用或者企业私有化部署感到烦恼？看看Qwen的Huggingface仓库找找答案吧，你想要的基本都有。但端上来的太多力，求求你饶了我吧（</p>\n<ul>\n<li>如果有2507变体，则代指2507而非2504版本。</li>\n<li>由于Qwen3家族的大语言模型实在太多，故拆分表格。</li>\n<li>30B-A3B体感类似于14B Dense模型。</li>\n<li>Qwen3 Next很好用，是中小企业部署的绝佳选择…如果不需要VL的话。</li>\n</ul>\n<p>Qwen3系列：</p>\n<table>\n<thead>\n<tr>\n<th>模型名</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Qwen3-235B-A22B-Thinking</strong></td>\n<td>MoE模型，思考模式，专注推理能力（很耗Token！）</td>\n</tr>\n<tr>\n<td><strong>Qwen3-235B-A22B-Instruct</strong></td>\n<td>MoE模型，指令微调版本（同样有输出思维链的倾向）</td>\n</tr>\n<tr>\n<td><strong>Qwen3-30B-A3B-Thinking</strong></td>\n<td>MoE模型，思考模式。非常适合资源有限的用户推理，激活3B就算在核显上也能跑的很快</td>\n</tr>\n<tr>\n<td><strong>Qwen3-30B-A3B-Instruct</strong></td>\n<td>MoE模型，指令微调版本，同上</td>\n</tr>\n<tr>\n<td><strong>Qwen3-32B</strong></td>\n<td>密集模型，性能强于30BA3B，但Dense不适合端侧推理</td>\n</tr>\n<tr>\n<td><strong>Qwen3-14B</strong></td>\n<td>密集模型，从这里开始大部分家用显卡也能爽玩</td>\n</tr>\n<tr>\n<td><strong>Qwen3-8B</strong></td>\n<td>密集模型</td>\n</tr>\n<tr>\n<td><strong>Qwen3-4B</strong></td>\n<td>密集模型，从这里开始视作端侧推理范畴</td>\n</tr>\n<tr>\n<td><strong>Qwen3-1.7B</strong></td>\n<td>密集模型</td>\n</tr>\n<tr>\n<td><strong>Qwen3-0.6B</strong></td>\n<td>密集模型，学术研究用居多</td>\n</tr>\n</tbody></table>\n<p>Qwen3-VL系列：</p>\n<table>\n<thead>\n<tr>\n<th>模型名</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Qwen3-VL-235B-A22B-Thinking</strong></td>\n<td>MoE视觉语言模型，思考模式，开源视觉模的神</td>\n</tr>\n<tr>\n<td><strong>Qwen3-VL-235B-A22B-Instruct</strong></td>\n<td>MoE视觉语言模型，指令版本，同上</td>\n</tr>\n<tr>\n<td><strong>Qwen3-VL-32B-Instruct</strong></td>\n<td>很少用</td>\n</tr>\n<tr>\n<td><strong>Qwen3-VL-30B-A3B-Instruct</strong></td>\n<td>MoE视觉语言模型，推理飞快，也是神</td>\n</tr>\n<tr>\n<td><strong>Qwen3-VL-8B-Instruct</strong></td>\n<td>密集语言模型，实际上已经能胜任大部分任务</td>\n</tr>\n<tr>\n<td><strong>Qwen3-VL-4B-Instruct</strong></td>\n<td>很适合端侧推理</td>\n</tr>\n<tr>\n<td><strong>Qwen3-VL-2B-Instruct</strong></td>\n<td>有点小了，学术研究用居多</td>\n</tr>\n</tbody></table>\n<p>Qwen3 Coder &amp; Next 系列：</p>\n<table>\n<thead>\n<tr>\n<th>模型名</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Qwen3-Coder-480B-A35B-Instruct</strong></td>\n<td>480B参数MoE代码生成模型，够用，最重要的是Qwen Coder CLI提供不要钱的。但非常烧Token，已经不如更新的GLM和MiniMax好用了</td>\n</tr>\n<tr>\n<td><strong>Qwen3-Coder-30B-A3B-Instruct</strong></td>\n<td>国企&#x2F;军工码农优选，很容易就能本地部署起来</td>\n</tr>\n<tr>\n<td><strong>Qwen3-Next-80B-A3B-Instruct</strong></td>\n<td>门控注意力实现的稀疏模型，总参数80B保证了世界知识的前提上只要3B激活，非常好用</td>\n</tr>\n<tr>\n<td><strong>Qwen3-Next-80B-A3B-Thinking</strong></td>\n<td>同上，思考模式版本。本地部署再也不怕Qwen烧Token导致慢的抠脚啦，3B激活推理飞快</td>\n</tr>\n</tbody></table>\n<h2 id=\"智谱、MiniMax、月之暗面\"><a href=\"#智谱、MiniMax、月之暗面\" class=\"headerlink\" title=\"智谱、MiniMax、月之暗面\"></a>智谱、MiniMax、月之暗面</h2><p>从六小龙到四小虎，其他的现在不管创新能力和作为消费者追求性价比来讲都逐渐的退出舞台了。</p>\n<p>2025年里这三家真可谓是猛猛发力，想通了A÷的模式其实是对的，都转头去做Coding Agent模型了。但为什么每次宣传的时候都要把Claude拉出来遛…给人一种某米某花某ov发布的时候总要蹭一下苹果的感觉。其实产品已经做的挺好了，再接再厉就是。</p>\n<p>先说智谱，前身是THUDM，属于从LLaMA时代一路走过来的老将了。当年本地部署最早能讲明白中文的就是<code>chatglm-6b</code>，给人留下了深刻印象，我也基于官方教程使用llama-factory微调过一个法律模型。可惜GLM时代一开始扭扭捏捏不肯开源，直到GLM4后期开始回过神来开始全面拥抱社区。GLM-4和GLM4-0414的牛刀小试给人足够好的印象，稳定低幻觉的输出非常适用于RAG场景。</p>\n<p>在各家都间歇性拉坨大的大背景下，GLM憋了个好活，从4.5到4.7一路高歌猛进，用上了358B的MoE。当然这个尺寸基本上就跟HomeLab和小企业没缘分了，然而Coding Agent的能力突飞猛进也是事实。假如你写代码的时候不被允许使用国外大模型的话，GLM家族是非常好的选择，和MiniMax互为竞争对手。</p>\n<table>\n<thead>\n<tr>\n<th>模型名</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>GLM-4.5</td>\n<td>358B-A32B，梦开始的地方</td>\n</tr>\n<tr>\n<td>GLM-4.5-Air</td>\n<td>GLM-4.5精简版，106B-A12B，非常适合中小企业部署使用，四张4090美美跑量化版</td>\n</tr>\n<tr>\n<td>GLM-4.5V</td>\n<td>带视觉的4.5 Air，但上下文长度受限</td>\n</tr>\n<tr>\n<td>GLM-4.6</td>\n<td>进一步增强了能力，相当够用</td>\n</tr>\n<tr>\n<td>GLM-4.6V</td>\n<td>其实就是带视觉的4.6 Air，只不过本代没出Air</td>\n</tr>\n<tr>\n<td>GLM-4.7</td>\n<td>最新的支持交错思考的旗舰型号</td>\n</tr>\n</tbody></table>\n<p>然后是MiniMax，跟智谱真是一对苦命鸳鸯，在港股赛跑上市，最终智谱领先一天。MiniMax在学习Claude的路上更加激进，仅用了230B参数就能够和GLM系列掰手腕，处于编码模型的T1梯队。MiniMax-M2引入了交错思考，配合Agent方面的训练，在代码工程上的能力得到了显著提升。而且由于激活参数相比GLM系列更小，在推理速度上优势尤为显著。<strong>需要注意的是，在实际工作中one shot是很难的，很多时候都需要手动测试然后提出问题再尝试修复BUG才能得到最终满意的结果，所以推理速度是很重要的一个评判指标</strong>。</p>\n<p>此外，MiniMax的音视频合成同样性价比很高，海外业务做的不错。</p>\n<table>\n<thead>\n<tr>\n<th>模型名</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MiniMax-M2</td>\n<td>230B-A10B</td>\n</tr>\n<tr>\n<td>MiniMax-M2.1</td>\n<td>进一步增强了代码能力</td>\n</tr>\n</tbody></table>\n<p>最后是月之暗面的Kimi系列。月之暗面其实之前的底模都很一般（K1.5时代），但产品和营销都做的不错。可以说国产AI里月之暗面的产品能够跟豆包一较高下，而网络搜索效果可能还要更胜一筹。</p>\n<p>有没有一种比DeepSeek更适合编码的DeepSeek？有的，那就是Kimi K2家族。K2在DeepSeek的架构上进行了Scaling up，用开源的1T巨模震撼了所有人的眼球。大参数量带来的泛化能力增强显而易见：情感更细腻，写代码也能够力大砖飞。后续K2趁热打铁推出了K2-Instruct-0905以进一步逼近Claude水平，还推出了带有思考模式的Kimi-K2-Thinking。够大的参数带来了更多的世界知识，不少冷门的编码场景K2也能够解决。优点说完了那缺点呢？1T巨模的计算成本实在是太高了，性价比和推理速度上是不如上面两家的。</p>\n<p>Kimi是少有的网页比API好用的厂，网络搜索和深度研究都不错：</p>\n<table>\n<thead>\n<tr>\n<th>模型名</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Kimi-K2-Instruct</td>\n<td>1T巨模，不带思考</td>\n</tr>\n<tr>\n<td>Kimi-K2-Instruct-0905</td>\n<td>进一步增强了代码能力</td>\n</tr>\n<tr>\n<td>Kimi-K2-Thinking</td>\n<td>带思考的版本</td>\n</tr>\n</tbody></table>\n<h2 id=\"腾讯、小米、美团：互联网厂赶晚集的救赎\"><a href=\"#腾讯、小米、美团：互联网厂赶晚集的救赎\" class=\"headerlink\" title=\"腾讯、小米、美团：互联网厂赶晚集的救赎\"></a>腾讯、小米、美团：互联网厂赶晚集的救赎</h2><p>看到其他厂做AI如火如荼，这几家也坐不住了。</p>\n<p>腾讯和字节有点像的是，都在音视频媒体&#x2F;多模态方向发力。但腾讯的LLM…就此略过，也就<code>Hunyuan-A13B</code>有点价值，在80B这个档可以和Qwen掰手腕。混元的3D一直做的不错。</p>\n<p>如果小米进场，那么这个时机一定已经成熟。小米在罗福莉加入后，MiMo家族也初露峥嵘。MiMo-7B及VL变体基本就是Qwen2.5上训练得来，没什么太多的可圈可点之处。但Miloco和MiMo-V2-Flash却收获了一些好评。Miloco是在MiMo-VL基础上再次训练得来的智能家居视觉模型，小尺寸保证了私有化部署的可行性，希望视觉大模型能够给智能家居再次注入一支强心针。而MiMo-V2-Flash是一款309B-A15B的MoE模型，前DeepSeek研究员携MiMo归来挑战老东家，V我50听复仇计划？目前MiMo V2处于免费阶段，也着重宣传了编码性能，但我更看希望看到的是MiMo在Agent结合米厂的各种智能硬件的落地探索，现在AI成功落地转化的场景太少了。</p>\n<table>\n<thead>\n<tr>\n<th>模型名</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Xiaomi-MiMo-VL-Miloco-7B</td>\n<td>视觉模型，私有化部署智能家居用</td>\n</tr>\n<tr>\n<td>MiMo-V2-Flash</td>\n<td>快速的编码&amp;Agent模型</td>\n</tr>\n</tbody></table>\n<p>美团LongCat：龙猫？长猫？这个名字有点难绷。LongCat-Flash虽说叫Flash，但实际上是一款560B-A27B的大模。美团也在音视频上发力，除了大语言模型外还有生图模型以及视频模型，有点黏着Qwen贴身搏斗的感觉。只谈大模型来说，LongCat感觉平平无奇，且待后面发展。比起MiMo Flash的0.7元输入&#x2F;2.1元输出来说，LongCat-Flash的5元输出就显得有些不值了。</p>\n<h2 id=\"谷歌、OpenAI：秀肌肉\"><a href=\"#谷歌、OpenAI：秀肌肉\" class=\"headerlink\" title=\"谷歌、OpenAI：秀肌肉\"></a>谷歌、OpenAI：秀肌肉</h2><p>俗话说，命里缺啥，名字里就得起啥，所以OpenAI不Open也非常合理。自从GPT-2后OpenAI就再也没放出LLM的权重，期间只是端出了Whisper。奥特曼在造了好几个月的势后，终于扭扭捏捏地放出了所谓堪比o3-mini的gpt-oss系列，可惜o3早已过气。而且gpt-oss真的能和o3-mini比肩吗？缺少了视觉能力、离谱的自我审查，还有着超高的幻觉，以至于LiveCodeBench上出现了gpt-oss-20b反杀gpt-oss-120b的奇观，因为120b版本的幻觉实在是太高了。</p>\n<p>当然，事物总是有两面性的。gpt-oss的放出给了大家许多启发，比如稀疏注意力、原生MXFP4、在思考中调用工具…总之，这是一个适合研究学习用的模型，而不太适合中国应用场景下的部署使用。</p>\n<table>\n<thead>\n<tr>\n<th>模型名</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>gpt-oss-20b</td>\n<td>原生MXFP4，很小很快，处理日常任务还挺好用</td>\n</tr>\n<tr>\n<td>gpt-oss-120b</td>\n<td>一坨，强烈不推荐。在Antigravity里和Claude 4.5 Opus并列有一种我和科比合砍83分的美感</td>\n</tr>\n</tbody></table>\n<p>到了谷歌这里又不太一样了：Gemma家族在国外社区还是一个不错的选项。Gemma 3虽然已经较老，但27B Dense结构带视觉在某些特定场景下可能也会有不错的效果，与Qwen3-VL可以掰掰手腕。尤其需要注意的是，Gemma 3有许多变体，比如医疗领域微调的medgemma、端侧使用的t5gemma、gemma-3n等。这里仅介绍主力Gemma 3家族，对端侧推理感兴趣的可以自行了解。</p>\n<p>Gemma 3比起Qwen 3家族的优势之一是谷歌给出了Gemma 3 QAT权重。量化感知训练版本比起传统PTQ来说，可以在量化到更低精度时保持较好的性能。这对于HomeLab玩家来说是一个好消息。如果你希望在家里部署一个性能尚可且带有视觉的大模型，你需要做出选择的就是Gemma 3还是Qwen 3。</p>\n<p>下方是原始指令对齐权重（其他变体请在Huggingface上自行查询）：</p>\n<table>\n<thead>\n<tr>\n<th>模型名</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>gemma-3-1b-it</td>\n<td>-</td>\n</tr>\n<tr>\n<td>gemma-3-4b-it</td>\n<td>-</td>\n</tr>\n<tr>\n<td>gemma-3-12b-it</td>\n<td>-</td>\n</tr>\n<tr>\n<td>gemma-3-27b-it</td>\n<td>-</td>\n</tr>\n</tbody></table>\n",
            "tags": [
                "LLM",
                "VLM"
            ]
        },
        {
            "id": "https://github.com/yangxiangnanwill/yangxiangnanwill.github.io/2026/01/04/%E5%A5%BD%E5%A5%BD%E7%A0%81%E4%BB%A3%E7%A0%81%E5%90%96/LLM/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E7%90%86%E8%AE%BA%E7%AE%80%E4%BB%8B/",
            "url": "https://github.com/yangxiangnanwill/yangxiangnanwill.github.io/2026/01/04/%E5%A5%BD%E5%A5%BD%E7%A0%81%E4%BB%A3%E7%A0%81%E5%90%96/LLM/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E7%90%86%E8%AE%BA%E7%AE%80%E4%BB%8B/",
            "title": "大型语言模型（LLM）理论简介",
            "date_published": "2026-01-04T02:06:38.333Z",
            "content_html": "<h1 id=\"什么是大型语言模型（LLM）\"><a href=\"#什么是大型语言模型（LLM）\" class=\"headerlink\" title=\"什么是大型语言模型（LLM）\"></a>什么是大型语言模型（LLM）</h1><h2 id=\"大型语言模型（LLM）的概念\"><a href=\"#大型语言模型（LLM）的概念\" class=\"headerlink\" title=\"大型语言模型（LLM）的概念\"></a>大型语言模型（LLM）的概念</h2><p>大语言模型（LLM，Large Language Model），也称大型语言模型，是一种旨在理解和生成人类语言的人工智能模型。</p>\n<p>LLM 通常指包含数百亿（或更多）参数的语言模型，它们在海量的文本数据上进行训练，从而获得对语言深层次的理解。目前，国外的知名 LLM 有 GPT-3.5、GPT-4、PaLM、Claude 和 LLaMA 等，国内的有文心一言、讯飞星火、通义千问、ChatGLM、百川等。</p>\n<p>为了探索性能的极限，许多研究人员开始训练越来越庞大的语言模型，例如拥有 1750 亿参数的 GPT-3 和 5400 亿参数的 PaLM 。尽管这些大型语言模型与小型语言模型（例如 3.3 亿参数的 BERT 和 15 亿参数的 GPT-2）使用相似的架构和预训练任务，但它们展现出截然不同的能力，尤其在解决复杂任务时表现出了惊人的潜力，这被称为“涌现能力”。以 GPT-3 和 GPT-2 为例，GPT-3 可以通过学习上下文来解决少样本任务，而 GPT-2 在这方面表现较差。因此，科研界给这些庞大的语言模型起了个名字，称之为“大语言模型（LLM）”。LLM 的一个杰出应用就是 ChatGPT ，它是 GPT 系列 LLM 用于与人类对话式应用的大胆尝试，展现出了非常流畅和自然的表现。</p>\n<h2 id=\"LLM-的发展历程\"><a href=\"#LLM-的发展历程\" class=\"headerlink\" title=\"LLM 的发展历程\"></a>LLM 的发展历程</h2><p>语言建模的研究可以追溯到20 世纪 90 年代，当时的研究主要集中在采用统计学习方法来预测词汇，通过分析前面的词汇来预测下一个词汇。但在理解复杂语言规则方面存在一定局限性。</p>\n<p>随后，研究人员不断尝试改进，2003 年深度学习先驱 Bengio 在他的经典论文 《A Neural Probabilistic Language Model》中，首次将深度学习的思想融入到语言模型中。强大的神经网络模型，相当于为计算机提供了强大的”大脑”来理解语言，让模型可以更好地捕捉和理解语言中的复杂关系。</p>\n<p>2018 年左右，Transformer 架构的神经网络模型开始崭露头角。通过大量文本数据训练这些模型，使它们能够通过阅读大量文本来深入理解语言规则和模式，就像让计算机阅读整个互联网一样，对语言有了更深刻的理解，极大地提升了模型在各种自然语言处理任务上的表现。</p>\n<p>与此同时，研究人员发现，随着语言模型规模的扩大（增加模型大小或使用更多数据），模型展现出了一些惊人的能力，在各种任务中的表现均显著提升。这一发现标志着大型语言模型（LLM）时代的开启。</p>\n<h2 id=\"常见的-LLM-模型\"><a href=\"#常见的-LLM-模型\" class=\"headerlink\" title=\"常见的 LLM 模型\"></a>常见的 LLM 模型</h2><p>主要介绍几个国内外常见的大模型（包括开源和闭源）。</p>\n<h3 id=\"闭源-LLM-未公开源代码\"><a href=\"#闭源-LLM-未公开源代码\" class=\"headerlink\" title=\"闭源 LLM (未公开源代码)\"></a>闭源 LLM (未公开源代码)</h3><h4 id=\"GPT-系列\"><a href=\"#GPT-系列\" class=\"headerlink\" title=\"GPT 系列\"></a>GPT 系列</h4><p>OpenAI 公司在 2018 年提出的 GPT（Generative Pre-Training） 模型是典型的 生成式预训练语言模型 之一。</p>\n<p>GPT 模型的基本原则是通过语言建模将世界知识压缩到仅解码器 (decoder-only) 的 Transformer 模型中，这样它就可以恢复(或记忆)世界知识的语义，并充当通用任务求解器。它能够成功的两个关键点：</p>\n<ul>\n<li>训练能够准确预测下一个单词的 decoder-only 的 Transformer 语言模型</li>\n<li>扩展语言模型的大小</li>\n</ul>\n<h5 id=\"ChatGPT\"><a href=\"#ChatGPT\" class=\"headerlink\" title=\"ChatGPT\"></a>ChatGPT</h5><blockquote>\n<p><a href=\"https://chat.openai.com/\">ChatGPT 使用地址</a></p>\n</blockquote>\n<p><code>2022 年 11 月</code>，<strong>OpenAI</strong> 发布了基于 GPT 模型（GPT-3.5 和 GPT-4） 的<strong>会话应用 ChatGPT</strong>。由于与人类交流的出色能力，ChatGPT 自发布以来就引发了人工智能社区的兴奋。ChatGPT 是基于强大的 GPT 模型开发的，具有特别优化的会话能力。</p>\n<p>ChatGPT 从本质上来说是一个 LLM 应用，是基于基座模型开发出来的，与基座模型有本质的区别。其支持 GPT-3.5 和 GPT-4 两个版本。</p>\n<p>现在的 ChatGPT 支持最长达 32,000 个字符，知识截止日期是 2021 年 9 月，它可以执行各种任务，包括<strong>代码编写、数学问题求解、写作建议</strong>等。ChatGPT 在与人类交流方面表现出了卓越的能力：拥有丰富的知识储备，对数学问题进行推理的技能，在多回合对话中准确追踪上下文，并且与人类安全使用的价值观非常一致。后来，ChatGPT 支持插件机制，这进一步扩展了 ChatGPT 与现有工具或应用程序的能力。到目前为止，它似乎是人工智能历史上最强大的聊天机器人。ChatGPT 的推出对未来的人工智能研究具有重大影响，它为探索类人人工智能系统提供了启示。</p>\n<h5 id=\"GPT-4\"><a href=\"#GPT-4\" class=\"headerlink\" title=\"GPT-4\"></a>GPT-4</h5><p><code>2023 年 3 月</code>发布的 GPT-4，它将<strong>文本输入扩展到多模态信号</strong>。GPT3.5 拥有 1750 亿 个参数，而 GPT4 的参数量官方并没有公布，但有相关人员猜测，GPT-4 在 120 层中总共包含了 1.8 万亿参数，也就是说，GPT-4 的规模是 GPT-3 的 10 倍以上。因此，GPT-4 比 GPT-3.5 <strong>解决复杂任务的能力更强，在许多评估任务上表现出较大的性能提升</strong>。</p>\n<p>最近的一项研究通过对人为生成的问题进行定性测试来研究 GPT-4 的能力，这些问题包含了各种各样的困难任务，并表明 GPT-4 可以比之前的 GPT 模型(如 GPT3.5 )实现更优越的性能。此外，由于六个月的迭代校准(在 RLHF 训练中有额外的安全奖励信号)，GPT-4 对恶意或挑衅性查询的响应更安全，并应用了一些干预策略来缓解 LLM 可能出现的问题，如幻觉、隐私和过度依赖。</p>\n<blockquote>\n<p>注意：2023 年 11 月 7 日， OpenAI 召开了首个开发者大会，会上推出了最新的大语言模型 GPT-4 Turbo，Turbo 相当于进阶版。它将上下文长度扩展到 128k，相当于 300 页文本，并且训练知识更新到 2023 年 4 月</p>\n</blockquote>\n<p>GPT3.5 是免费的，而 GPT-4 是收费的。需要开通 plus 会员 20 美元&#x2F;月。</p>\n<h4 id=\"Claude系列\"><a href=\"#Claude系列\" class=\"headerlink\" title=\"Claude系列\"></a>Claude系列</h4><p>Claude 系列模型是由 OpenAI 离职人员创建的 <strong>Anthropic</strong> 公司开发的闭源语言大模型。</p>\n<blockquote>\n<p><a href=\"https://claude.ai/chats\">Claude 使用地址</a></p>\n</blockquote>\n<p>最早的 <strong>Claude</strong> 于 <code>2023 年 3 月 15 日</code>发布，在 2023 年 7 月 11 日，更新至 <strong>Claude-2</strong>， 并在 <code>2024 年 3 月 4 日</code>更新至 <strong>Claude-3</strong>。</p>\n<p>Claude 3 系列包括三个不同的模型，分别是 Claude 3 Haiku、Claude 3 Sonnet 和 Claude 3 Opus，它们的能力依次递增，旨在满足不同用户和应用场景的需求。</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">模型名称</th>\n<th align=\"center\">上下文长度</th>\n<th align=\"center\">特点</th>\n<th align=\"center\">input 费用($&#x2F;1M tokens)</th>\n<th align=\"center\">output 费用($&#x2F;1M tokens)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">Claude 3 Haiku</td>\n<td align=\"center\">200k</td>\n<td align=\"center\">速度最快</td>\n<td align=\"center\">0.25</td>\n<td align=\"center\">1.25</td>\n</tr>\n<tr>\n<td align=\"center\">Claude 3 Sonnet</td>\n<td align=\"center\">200k</td>\n<td align=\"center\">平衡</td>\n<td align=\"center\">3git</td>\n<td align=\"center\">15</td>\n</tr>\n<tr>\n<td align=\"center\">Claude 3 Opus</td>\n<td align=\"center\">200k</td>\n<td align=\"center\">性能最强</td>\n<td align=\"center\">15</td>\n<td align=\"center\">75</td>\n</tr>\n</tbody></table>\n<h4 id=\"PaLM-x2F-Gemini-系列\"><a href=\"#PaLM-x2F-Gemini-系列\" class=\"headerlink\" title=\"PaLM&#x2F;Gemini 系列\"></a>PaLM&#x2F;Gemini 系列</h4><p><strong>PaLM 系列</strong>语言大模型由 <strong>Google</strong> 开发。其初始版本于 <code>2022 年 4 月</code>发布，并在 2023 年 3 月公开了 API。2023 年 5 月，Google 发布了 <strong>PaLM 2</strong>，<code>2024 年 2 月 1 日</code>，Google 将 Bard(之前发布的对话应用) 的底层大模型驱动由 PaLM2 更改为 <strong>Gemini</strong>，同时也将原先的 Bard 更名为 <strong>Gemini</strong>。</p>\n<blockquote>\n<p><a href=\"https://ai.google/discover/palm2/\">PaLM 官方地址</a></p>\n</blockquote>\n<blockquote>\n<p><a href=\"https://gemini.google.com/\">Gemini 使用地址</a></p>\n</blockquote>\n<p>目前的 Gemini 是第一个版本，即 Gemini 1.0，根据参数量不同分为 Ultra, Pro 和 Nano 三个版本。</p>\n<h4 id=\"文心一言\"><a href=\"#文心一言\" class=\"headerlink\" title=\"文心一言\"></a>文心一言</h4><blockquote>\n<p><a href=\"https://yiyan.baidu.com/\">文心一言使用地址</a></p>\n</blockquote>\n<p><strong>文心一言是基于百度文心大模型的知识增强语言大模型</strong>，于 <code>2023 年 3 月</code>在国内率先开启邀测。文心一言的基础模型文心大模型于 2019 年发布 1.0 版，现已更新到 <strong>4.0</strong> 版本。更进一步划分，文心大模型包括 NLP 大模型、CV 大模型、跨模态大模型、生物计算大模型、行业大模型。中文能力相对来说非常不错的闭源模型。</p>\n<p>文心一言网页版分为<strong>免费版</strong>和<strong>专业版</strong>。</p>\n<ul>\n<li>免费版使用文心 3.5 版本，已经能够满足个人用户或小型企业的大部分需求。</li>\n<li>专业版使用文心 4.0 版本。定价为 59.9 元&#x2F;月，连续包月优惠价为 49.9 元&#x2F;月</li>\n</ul>\n<p>同时也可以使用 API 进行调用（<a href=\"https://console.bce.baidu.com/qianfan/chargemanage/list\">计费详情</a>）。</p>\n<h4 id=\"星火大模型\"><a href=\"#星火大模型\" class=\"headerlink\" title=\"星火大模型\"></a>星火大模型</h4><blockquote>\n<p><a href=\"https://xinghuo.xfyun.cn/\">星火大模型使用地址</a></p>\n</blockquote>\n<p><strong>讯飞星火认知大模型</strong>是<strong>科大讯飞</strong>发布的语言大模型，支持多种自然语言处理任务。该模型于 <code>2023 年 5 月</code>首次发布，后续经过多次升级。<code>2023 年 10 月</code>，讯飞发布了<strong>讯飞星火认知大模型 V3.0</strong>。<code>2024 年 1 月</code>，讯飞发布了<strong>讯飞星火认知大模型 V3.5</strong>，在语言理解，文本生成，知识问答等七个方面进行了升级，并且支持 system 指令，插件调用等多项功能。</p>\n<h3 id=\"开源LLM\"><a href=\"#开源LLM\" class=\"headerlink\" title=\"开源LLM\"></a>开源LLM</h3><h4 id=\"LLaMA-系列\"><a href=\"#LLaMA-系列\" class=\"headerlink\" title=\"LLaMA 系列\"></a>LLaMA 系列</h4><blockquote>\n<p><a href=\"https://llama.meta.com/\">LLaMA 官方地址</a></p>\n</blockquote>\n<blockquote>\n<p><a href=\"https://github.com/facebookresearch/llama\">LLaMA 开源地址</a></p>\n</blockquote>\n<p><strong>LLaMA 系列模型</strong>是 <strong>Meta</strong> 开源的一组参数规模 <strong>从 7B 到 70B</strong> 的基础语言模型。LLaMA 于<code>2023 年 2 月</code>发布，并于 <code>2023 年 7 月</code>发布了 <strong>LLaMA2</strong> 模型。它们都是在数万亿个字符上训练的，展示了如何<strong>仅使用公开可用的数据集来训练最先进的模型</strong>，而不需要依赖专有或不可访问的数据集。这些数据集包括 Common Crawl、Wikipedia、OpenWebText2、RealNews、Books 等。LLaMA 模型使用了<strong>大规模的数据过滤和清洗技术</strong>，以提高数据质量和多样性，减少噪声和偏见。LLaMA 模型还使用了高效的<strong>数据并行</strong>和<strong>流水线并行</strong>技术，以加速模型的训练和扩展。特别地，LLaMA 13B 在 CommonsenseQA 等 9 个基准测试中超过了 GPT-3 (175B)，而 <strong>LLaMA 65B 与最优秀的模型 Chinchilla-70B 和 PaLM-540B 相媲美</strong>。LLaMA 通过使用更少的字符来达到最佳性能，从而在各种推理预算下具有优势。</p>\n<p>与 GPT 系列相同，LLaMA 模型也采用了 <strong>decoder-only</strong> 架构，同时结合了一些前人工作的改进：</p>\n<ul>\n<li><code>Pre-normalization 正则化</code>：为了提高训练稳定性，LLaMA 对每个 Transformer 子层的输入进行了 RMSNorm 归一化，这种归一化方法可以避免梯度爆炸和消失的问题，提高模型的收敛速度和性能；</li>\n<li><code>SwiGLU 激活函数</code>：将 ReLU 非线性替换为 SwiGLU 激活函数，增加网络的表达能力和非线性，同时减少参数量和计算量；</li>\n<li><code>旋转位置编码（RoPE，Rotary Position Embedding）</code>：模型的输入不再使用位置编码，而是在网络的每一层添加了位置编码，RoPE 位置编码可以有效地捕捉输入序列中的相对位置信息，并且具有更好的泛化能力。</li>\n</ul>\n<p><strong>LLaMA2</strong> 在 LLaMA 系列模型的基础上进行了改进，提高了模型的性能和效率：</p>\n<ul>\n<li><code>更多的训练数据量</code>：LLaMA2 在 2 万亿个 token 的数据上进行预训练，相比 LLaMA1 的训练数据量增加了 40%。LLaMA2 能够接触到更多的文本信息，从而提高了其理解和生成文本的能力。</li>\n<li><code>更长的上下文长度</code>：LLaMA2 的上下文长度增加了一倍，从 LLaMA1 的 2048 个 token 增加到了 4096。这使得 LLaMA2 能够处理更长的文本序列，改善了对长文本的理解和生成能力。</li>\n<li><code>分组查询注意力（GQA，Grouped-Query Attention）</code>：通过将查询（query）分组并在组内共享键（key）和值（value），减少了计算量，同时保持了模型性能，提高了大型模型的推理效率。</li>\n</ul>\n<h4 id=\"通义千问\"><a href=\"#通义千问\" class=\"headerlink\" title=\"通义千问\"></a>通义千问</h4><blockquote>\n<p><a href=\"https://tongyi.aliyun.com/\">通义千问使用地址</a></p>\n</blockquote>\n<blockquote>\n<p><a href=\"https://github.com/QwenLM/Qwen1.5\">通义千问开源地址</a></p>\n</blockquote>\n<p><strong>通义千问由阿里巴巴基于“通义”大模型研发</strong>，于 <code>2023 年 4 月</code>正式发布。2023 年 9 月，阿里云开源了 Qwen（通义千问）系列工作。并于 <code>2024 年 2 月 5 日</code>，开源了 <strong>Qwen1.5</strong>（Qwen2 的测试版）是一个 <strong>decoder-Only</strong> 的模型，采用 <code>SwiGLU 激活</code>、<code>RoPE</code>、<code>multi-head attention</code>的架构。中文能力相对来说非常不错的闭源模型。</p>\n<p>目前，已经开源了 7 种模型大小：<strong>0.5B、1.8B、4B、7B、14B 、72B 的 Dense 模型和 14B (A2.7B)的 MoE 模型</strong>；所有模型均支持长度为 <strong>32768 token</strong> 的上下文；</p>\n<h4 id=\"GLM-系列\"><a href=\"#GLM-系列\" class=\"headerlink\" title=\"GLM 系列\"></a>GLM 系列</h4><blockquote>\n<p><a href=\"https://chatglm.cn/\">ChatGLM 使用地址</a></p>\n</blockquote>\n<blockquote>\n<p><a href=\"https://github.com/THUDM\">ChatGLM 开源地址</a></p>\n</blockquote>\n<p><strong>GLM 系列模型</strong>是<strong>清华大学和智谱 AI 等</strong>合作研发的语言大模型。2023 年 3 月 发布了 <strong>ChatGLM</strong>。2023 年 6 月发布了 <strong>ChatGLM 2</strong>。2023 年 10 月推出了 <strong>ChatGLM3</strong>。</p>\n<p><strong>ChatGLM3-6B</strong> 支持正常的多轮对话的同时，原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景。</p>\n<p>开源了<code>对话模型</code> <strong>ChatGLM3-6B</strong>、<code>基础模型</code> <strong>ChatGLM3-6B-Base</strong>、<code>长文本对话模型</code> <strong>ChatGLM3-6B-32K</strong>、<code>多模态</code> <strong>CogVLM-17B</strong> 、以及 <code>智能体</code> <strong>AgentLM</strong> 等全面对标 OpenAI。</p>\n<h4 id=\"Baichuan-系列\"><a href=\"#Baichuan-系列\" class=\"headerlink\" title=\"Baichuan 系列\"></a>Baichuan 系列</h4><blockquote>\n<p><a href=\"https://www.baichuan-ai.com/chat\">百川使用地址</a></p>\n</blockquote>\n<blockquote>\n<p><a href=\"https://github.com/baichuan-inc\">百川开源地址</a></p>\n</blockquote>\n<p><strong>Baichuan</strong> 是由<strong>百川智能</strong>开发的<strong>开源可商用</strong>的语言大模型。其基于<strong>Transformer 解码器架构（decoder-only）</strong>。</p>\n<p>2023 年 6 月 15 日发布了 <strong>Baichuan-7B</strong> 和 <strong>Baichuan-13B</strong>。百川同时开源了<strong>预训练</strong>和<strong>对齐</strong>模型，<code>预训练模型是面向开发者的“基座”</code>，而<code>对齐模型则面向广大需要对话功能的普通用户</code>。</p>\n<p><strong>Baichuan2</strong> 于 <code>2023年 9 月 6 日</code>推出。发布了 <strong>7B、13B</strong> 的 <strong>Base</strong> 和 <strong>Chat</strong> 版本，并提供了 Chat 版本的 <strong>4bits 量化</strong>。</p>\n<p><code>2024 年 1 月 29 日</code> 发布了 <strong>Baichuan 3</strong>。但是<strong>目前还没有开源</strong>。</p>\n",
            "tags": [
                "LLM",
                "人工智能"
            ]
        }
    ]
}